{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n\n<h1 align=center><font size = 5>Pre-Trained Models</font></h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Introduction\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Table of Contents\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<font size = 3> \n    \n1. <a href=\"#item31\">Import Libraries and Packages</a>\n2. <a href=\"#item32\">Download Data</a>  \n3. <a href=\"#item33\">Define Global Constants</a>  \n4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n5. <a href=\"#item35\">Compile and Fit Model</a>\n\n</font>\n    \n</div>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "   "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item31'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Import Libraries and Packages"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Let's start the lab by importing the libraries that we will be using in this lab."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n"
                }
            ],
            "source": "from keras.preprocessing.image import ImageDataGenerator"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library."
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "from keras.applications import ResNet50\nfrom keras.applications.resnet50 import preprocess_input"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item32'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Download Data"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed."
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-08-07 15:54:21--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 261482368 (249M) [application/zip]\nSaving to: \u2018concrete_data_week3.zip\u2019\n\n100%[======================================>] 261,482,368 39.1MB/s   in 6.3s   \n\n2020-08-07 15:54:28 (39.6 MB/s) - \u2018concrete_data_week3.zip\u2019 saved [261482368/261482368]\n\n"
                }
            ],
            "source": "## get the data\n!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                },
                "scrolled": true
            },
            "outputs": [],
            "source": "!unzip -q -o concrete_data_week3.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50*** error. So please **DO NOT DO IT**."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item33'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Define Global Constants"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Here, we will define constants that we will be using throughout the rest of the lab. \n\n1. We are obviously dealing with two classes, so *num_classes* is 2. \n2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n3. We will training and validating the model using batches of 100 images."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "num_classes = 2\nimage_resize = 224\nbatch_size_training = 100\nbatch_size_validation = 100"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item34'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Construct ImageDataGenerator Instances"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "data_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Next, we will use the *flow_from_directory* method to get the training images as follows:"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Found 30001 images belonging to 2 classes.\n"
                }
            ],
            "source": "train_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/train',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_training,\n    class_mode='categorical')"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**."
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Found 10001 images belonging to 2 classes.\n"
                }
            ],
            "source": "## Type your answer here\n\nvalidation_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/valid',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_validation,\n    class_mode='categorical')"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Double-click __here__ for the solution.\n<!-- The correct answer is:\nvalidation_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/valid',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_validation,\n    class_mode='categorical')\n-->\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item35'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Build, Compile and Fit Model"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In this section, we will start building our model. We will use the Sequential model class from Keras."
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model = Sequential()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**."
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n94658560/94653016 [==============================] - 1s 0us/step\n"
                }
            ],
            "source": "model.add(ResNet50(\n    include_top=False,\n    pooling='avg',\n    weights='imagenet',\n    ))"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function."
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.add(Dense(num_classes, activation='softmax'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "You can access the model's layers using the *layers* attribute of our model object. "
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[<keras.engine.training.Model at 0x7f29585eeef0>,\n <keras.layers.core.Dense at 0x7f28c432a198>]"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.layers"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "You can access the ResNet50 layers by running the following:"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                },
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[<keras.engine.input_layer.InputLayer at 0x7f2a6c65cb70>,\n <keras.layers.convolutional.ZeroPadding2D at 0x7f2a6c65cf98>,\n <keras.layers.convolutional.Conv2D at 0x7f2a6c65ce10>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a6c66b470>,\n <keras.layers.core.Activation at 0x7f2a6c66b898>,\n <keras.layers.convolutional.ZeroPadding2D at 0x7f2a6c685b38>,\n <keras.layers.pooling.MaxPooling2D at 0x7f2a6c685c18>,\n <keras.layers.convolutional.Conv2D at 0x7f2a6e6b0da0>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a6c041438>,\n <keras.layers.core.Activation at 0x7f2a6c041f60>,\n <keras.layers.convolutional.Conv2D at 0x7f2a5c7d3d30>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a5c7b5e80>,\n <keras.layers.core.Activation at 0x7f2a5c7589e8>,\n <keras.layers.convolutional.Conv2D at 0x7f2a5c6a5080>,\n <keras.layers.convolutional.Conv2D at 0x7f2a5c60c390>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a5c641b38>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a5c4f9828>,\n <keras.layers.merge.Add at 0x7f2a5c3fa940>,\n <keras.layers.core.Activation at 0x7f2a5c41b0b8>,\n <keras.layers.convolutional.Conv2D at 0x7f2a5c41b588>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a5c384208>,\n <keras.layers.core.Activation at 0x7f2a5c30f9e8>,\n <keras.layers.convolutional.Conv2D at 0x7f2a5c2210b8>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a5c2f5c50>,\n <keras.layers.core.Activation at 0x7f2a5c23f5c0>,\n <keras.layers.convolutional.Conv2D at 0x7f2a5c193550>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a5c174390>,\n <keras.layers.merge.Add at 0x7f2a6c67ab38>,\n <keras.layers.core.Activation at 0x7f2a5c049fd0>,\n <keras.layers.convolutional.Conv2D at 0x7f2a5c04f630>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a3878d3c8>,\n <keras.layers.core.Activation at 0x7f2a38766be0>,\n <keras.layers.convolutional.Conv2D at 0x7f2a386dacc0>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a3869fac8>,\n <keras.layers.core.Activation at 0x7f2a38643978>,\n <keras.layers.convolutional.Conv2D at 0x7f2a385eff60>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a385a9b38>,\n <keras.layers.merge.Add at 0x7f2a38577c88>,\n <keras.layers.core.Activation at 0x7f2a3844a160>,\n <keras.layers.convolutional.Conv2D at 0x7f2a3844a7b8>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a3842ff28>,\n <keras.layers.core.Activation at 0x7f2a383aa668>,\n <keras.layers.convolutional.Conv2D at 0x7f2a382ff710>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a382e2550>,\n <keras.layers.core.Activation at 0x7f2a3828b278>,\n <keras.layers.convolutional.Conv2D at 0x7f2a381bdf28>,\n <keras.layers.convolutional.Conv2D at 0x7f2a3813a550>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a38198828>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a380788d0>,\n <keras.layers.merge.Add at 0x7f2a38054e48>,\n <keras.layers.core.Activation at 0x7f2a18763be0>,\n <keras.layers.convolutional.Conv2D at 0x7f2a18763dd8>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a186f29e8>,\n <keras.layers.core.Activation at 0x7f2a18677128>,\n <keras.layers.convolutional.Conv2D at 0x7f2a18580f28>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a185dde10>,\n <keras.layers.core.Activation at 0x7f2a185a9ef0>,\n <keras.layers.convolutional.Conv2D at 0x7f2a1847c828>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a1845d320>,\n <keras.layers.merge.Add at 0x7f2a18404160>,\n <keras.layers.core.Activation at 0x7f2a183b26d8>,\n <keras.layers.convolutional.Conv2D at 0x7f2a183b2cf8>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a182bb358>,\n <keras.layers.core.Activation at 0x7f2a18293ba8>,\n <keras.layers.convolutional.Conv2D at 0x7f2a18207c88>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a18221828>,\n <keras.layers.core.Activation at 0x7f2a181ef9b0>,\n <keras.layers.convolutional.Conv2D at 0x7f2a18081b70>,\n <keras.layers.normalization.BatchNormalization at 0x7f2a180dab70>,\n <keras.layers.merge.Add at 0x7f2a180a7c50>,\n <keras.layers.core.Activation at 0x7f29f87b6080>,\n <keras.layers.convolutional.Conv2D at 0x7f29f87b67f0>,\n <keras.layers.normalization.BatchNormalization at 0x7f29f871cf60>,\n <keras.layers.core.Activation at 0x7f29f869b630>,\n <keras.layers.convolutional.Conv2D at 0x7f29f866e6a0>,\n <keras.layers.normalization.BatchNormalization at 0x7f29f85d3518>,\n <keras.layers.core.Activation at 0x7f29f8579240>,\n <keras.layers.convolutional.Conv2D at 0x7f29f852eef0>,\n <keras.layers.normalization.BatchNormalization at 0x7f29f84867b8>,\n <keras.layers.merge.Add at 0x7f29f84ad4e0>,\n <keras.layers.core.Activation at 0x7f29f83dd550>,\n <keras.layers.convolutional.Conv2D at 0x7f29f8344a90>,\n <keras.layers.normalization.BatchNormalization at 0x7f29f83697f0>,\n <keras.layers.core.Activation at 0x7f29f82f0780>,\n <keras.layers.convolutional.Conv2D at 0x7f29f8274f60>,\n <keras.layers.normalization.BatchNormalization at 0x7f29f824fd68>,\n <keras.layers.core.Activation at 0x7f29f821dd68>,\n <keras.layers.convolutional.Conv2D at 0x7f29f8173438>,\n <keras.layers.convolutional.Conv2D at 0x7f29f80d1630>,\n <keras.layers.normalization.BatchNormalization at 0x7f29f81102e8>,\n <keras.layers.normalization.BatchNormalization at 0x7f29d8772278>,\n <keras.layers.merge.Add at 0x7f29d874c6a0>,\n <keras.layers.core.Activation at 0x7f29d8603ac8>,\n <keras.layers.convolutional.Conv2D at 0x7f29d865a438>,\n <keras.layers.normalization.BatchNormalization at 0x7f29d862d978>,\n <keras.layers.core.Activation at 0x7f29d85a45f8>,\n <keras.layers.convolutional.Conv2D at 0x7f29d85786d8>,\n <keras.layers.normalization.BatchNormalization at 0x7f29d84db4e0>,\n <keras.layers.core.Activation at 0x7f29d8483208>,\n <keras.layers.convolutional.Conv2D at 0x7f29d8435ef0>,\n <keras.layers.normalization.BatchNormalization at 0x7f29d838e7f0>,\n <keras.layers.merge.Add at 0x7f29d834ee80>,\n <keras.layers.core.Activation at 0x7f29d824bac8>,\n <keras.layers.convolutional.Conv2D at 0x7f29d82a2438>,\n <keras.layers.normalization.BatchNormalization at 0x7f29d8270978>,\n <keras.layers.core.Activation at 0x7f29d81ee5f8>,\n <keras.layers.convolutional.Conv2D at 0x7f29d81406a0>,\n <keras.layers.normalization.BatchNormalization at 0x7f29d81224e0>,\n <keras.layers.core.Activation at 0x7f29d80cb208>,\n <keras.layers.convolutional.Conv2D at 0x7f29b87bceb8>,\n <keras.layers.normalization.BatchNormalization at 0x7f29b87947b8>,\n <keras.layers.merge.Add at 0x7f29b873b4e0>,\n <keras.layers.core.Activation at 0x7f29b8650a90>,\n <keras.layers.convolutional.Conv2D at 0x7f29b86a9400>,\n <keras.layers.normalization.BatchNormalization at 0x7f29b8677940>,\n <keras.layers.core.Activation at 0x7f29b85f25c0>,\n <keras.layers.convolutional.Conv2D at 0x7f29b8548668>,\n <keras.layers.normalization.BatchNormalization at 0x7f29b85454a8>,\n <keras.layers.core.Activation at 0x7f29b84d3278>,\n <keras.layers.convolutional.Conv2D at 0x7f29b8402e80>,\n <keras.layers.normalization.BatchNormalization at 0x7f29b83de780>,\n <keras.layers.merge.Add at 0x7f29b8381550>,\n <keras.layers.core.Activation at 0x7f29b82d3c50>,\n <keras.layers.convolutional.Conv2D at 0x7f29b82f2128>,\n <keras.layers.normalization.BatchNormalization at 0x7f29b823f908>,\n <keras.layers.core.Activation at 0x7f29b81b9588>,\n <keras.layers.convolutional.Conv2D at 0x7f29b818e630>,\n <keras.layers.normalization.BatchNormalization at 0x7f29b8170438>,\n <keras.layers.core.Activation at 0x7f29b8117208>,\n <keras.layers.convolutional.Conv2D at 0x7f29b8049e48>,\n <keras.layers.normalization.BatchNormalization at 0x7f29987e6748>,\n <keras.layers.merge.Add at 0x7f299878b470>,\n <keras.layers.core.Activation at 0x7f29986ba4e0>,\n <keras.layers.convolutional.Conv2D at 0x7f29986babe0>,\n <keras.layers.normalization.BatchNormalization at 0x7f2998644860>,\n <keras.layers.core.Activation at 0x7f29985c0550>,\n <keras.layers.convolutional.Conv2D at 0x7f29985945f8>,\n <keras.layers.normalization.BatchNormalization at 0x7f2998578438>,\n <keras.layers.core.Activation at 0x7f299851e208>,\n <keras.layers.convolutional.Conv2D at 0x7f299844de10>,\n <keras.layers.normalization.BatchNormalization at 0x7f2998429710>,\n <keras.layers.merge.Add at 0x7f29983d0438>,\n <keras.layers.core.Activation at 0x7f29982ff4a8>,\n <keras.layers.convolutional.Conv2D at 0x7f29982e6e48>,\n <keras.layers.normalization.BatchNormalization at 0x7f299828b710>,\n <keras.layers.core.Activation at 0x7f2998213518>,\n <keras.layers.convolutional.Conv2D at 0x7f29981de550>,\n <keras.layers.normalization.BatchNormalization at 0x7f2998142400>,\n <keras.layers.core.Activation at 0x7f29981661d0>,\n <keras.layers.convolutional.Conv2D at 0x7f2998099dd8>,\n <keras.layers.convolutional.Conv2D at 0x7f29787db400>,\n <keras.layers.normalization.BatchNormalization at 0x7f29980736d8>,\n <keras.layers.normalization.BatchNormalization at 0x7f29786eee10>,\n <keras.layers.merge.Add at 0x7f29786926a0>,\n <keras.layers.core.Activation at 0x7f29785e5080>,\n <keras.layers.convolutional.Conv2D at 0x7f29785e5668>,\n <keras.layers.normalization.BatchNormalization at 0x7f297854cc50>,\n <keras.layers.core.Activation at 0x7f29784c9a58>,\n <keras.layers.convolutional.Conv2D at 0x7f29784a1fd0>,\n <keras.layers.normalization.BatchNormalization at 0x7f29783fd978>,\n <keras.layers.core.Activation at 0x7f29784256a0>,\n <keras.layers.convolutional.Conv2D at 0x7f2978335f60>,\n <keras.layers.normalization.BatchNormalization at 0x7f297830e9e8>,\n <keras.layers.merge.Add at 0x7f29782e0b70>,\n <keras.layers.core.Activation at 0x7f297822a080>,\n <keras.layers.convolutional.Conv2D at 0x7f297822a668>,\n <keras.layers.normalization.BatchNormalization at 0x7f2978195c18>,\n <keras.layers.core.Activation at 0x7f2978114a58>,\n <keras.layers.convolutional.Conv2D at 0x7f29780e9f98>,\n <keras.layers.normalization.BatchNormalization at 0x7f2978044940>,\n <keras.layers.core.Activation at 0x7f297806d710>,\n <keras.layers.convolutional.Conv2D at 0x7f29586bbf28>,\n <keras.layers.normalization.BatchNormalization at 0x7f29587159b0>,\n <keras.layers.merge.Add at 0x7f29586e5b38>,\n <keras.layers.core.Activation at 0x7f2958632208>,\n <keras.layers.pooling.GlobalAveragePooling2D at 0x7f2958632630>]"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.layers[0].layers"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following."
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.layers[0].trainable = False"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer."
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nresnet50 (Model)             (None, 2048)              23587712  \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 4098      \n=================================================================\nTotal params: 23,591,810\nTrainable params: 4,098\nNon-trainable params: 23,587,712\n_________________________________________________________________\n"
                }
            ],
            "source": "model.summary()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Next we compile our model using the **adam** optimizer."
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "#steps_per_epoch_training = len(train_generator)\nsteps_per_epoch_training = 100\n#steps_per_epoch_validation = len(validation_generator)\nsteps_per_epoch_validation = 100\nnum_epochs = 1"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method."
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/1\n100/100 [==============================] - 9224s 92s/step - loss: 0.0884 - acc: 0.9699 - val_loss: 0.3499 - val_acc: 0.8345\n"
                }
            ],
            "source": "fit_history = model.fit_generator(\n    train_generator,\n    steps_per_epoch=steps_per_epoch_training,\n    epochs=num_epochs,\n    validation_data=validation_generator,\n    validation_steps=steps_per_epoch_validation,\n    verbose=1,\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Now that the model is trained, you are ready to start using it to classify images."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model."
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.save('classifier_resnet_model.h5')"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "# START CODE BLOCK\n# cos2file - takes an object from Cloud Object Storage and writes it to file on container file system.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: project object defined in project token\n# data_path: the directory to write the file\n# filename: name of the file in COS\n\nimport os\ndef cos2file(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n    open( data_dir + '/' + filename, 'wb').write(p.get_file(filename).read())\n\n# file2cos - takes file on container file system and writes it to an object in Cloud Object Storage.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: prooject object defined in project token\n# data_path: the directory to read the file from\n# filename: name of the file on container file system\n\nimport os\ndef file2cos(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    path_to_file = data_dir + '/' + filename\n    if os.path.exists(path_to_file):\n        file_object = open(path_to_file, 'rb')\n        p.save_data(filename, file_object, set_project_asset=True, overwrite=True)\n    else:\n        print(\"file2cos error: File not found\")\n# END CODE BLOCK"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "file2cos(project, '/work', 'classifier_resnet_model.h5')"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "### Thank you for completing this lab!\n\nThis notebook was created by Alex Aklson. I hope you found this lab interesting and educational."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1)."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<hr>\n\nCopyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}