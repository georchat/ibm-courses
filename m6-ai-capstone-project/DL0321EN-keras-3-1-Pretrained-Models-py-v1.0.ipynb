{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n\n<h1 align=center><font size = 5>Pre-Trained Models</font></h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Introduction\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Table of Contents\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<font size = 3> \n    \n1. <a href=\"#item31\">Import Libraries and Packages</a>\n2. <a href=\"#item32\">Download Data</a>  \n3. <a href=\"#item33\">Define Global Constants</a>  \n4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n5. <a href=\"#item35\">Compile and Fit Model</a>\n\n</font>\n    \n</div>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "   "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item31'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Import Libraries and Packages"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Let's start the lab by importing the libraries that we will be using in this lab."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n"
                }
            ],
            "source": "from keras.preprocessing.image import ImageDataGenerator"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library."
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "from keras.applications import ResNet50\nfrom keras.applications.resnet50 import preprocess_input"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item32'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Download Data"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed."
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-08-07 11:12:17--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 261482368 (249M) [application/zip]\nSaving to: \u2018concrete_data_week3.zip.2\u2019\n\n100%[======================================>] 261,482,368 35.2MB/s   in 6.5s   \n\n2020-08-07 11:12:24 (38.5 MB/s) - \u2018concrete_data_week3.zip.2\u2019 saved [261482368/261482368]\n\n"
                }
            ],
            "source": "## get the data\n!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                },
                "scrolled": true
            },
            "outputs": [],
            "source": "!unzip -q -o concrete_data_week3.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50*** error. So please **DO NOT DO IT**."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item33'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Define Global Constants"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Here, we will define constants that we will be using throughout the rest of the lab. \n\n1. We are obviously dealing with two classes, so *num_classes* is 2. \n2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n3. We will training and validating the model using batches of 100 images."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "num_classes = 2\nimage_resize = 224\nbatch_size_training = 100\nbatch_size_validation = 100"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item34'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Construct ImageDataGenerator Instances"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "data_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Next, we will use the *flow_from_directory* method to get the training images as follows:"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Found 30001 images belonging to 2 classes.\n"
                }
            ],
            "source": "train_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/train',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_training,\n    class_mode='categorical')"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**."
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Found 10001 images belonging to 2 classes.\n"
                }
            ],
            "source": "## Type your answer here\n\nvalidation_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/valid',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_validation,\n    class_mode='categorical')"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Double-click __here__ for the solution.\n<!-- The correct answer is:\nvalidation_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/valid',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_validation,\n    class_mode='categorical')\n-->\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item35'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Build, Compile and Fit Model"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In this section, we will start building our model. We will use the Sequential model class from Keras."
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model = Sequential()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**."
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"
                }
            ],
            "source": "model.add(ResNet50(\n    include_top=False,\n    pooling='avg',\n    weights='imagenet',\n    ))"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function."
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.add(Dense(num_classes, activation='softmax'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "You can access the model's layers using the *layers* attribute of our model object. "
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[<keras.engine.training.Model at 0x7f4c4c1ccc18>,\n <keras.layers.core.Dense at 0x7f4bb03220b8>]"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.layers"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "You can access the ResNet50 layers by running the following:"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                },
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[<keras.engine.input_layer.InputLayer at 0x7f4d542658d0>,\n <keras.layers.convolutional.ZeroPadding2D at 0x7f4d54265a20>,\n <keras.layers.convolutional.Conv2D at 0x7f4d54265ba8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d542721d0>,\n <keras.layers.core.Activation at 0x7f4d542724a8>,\n <keras.layers.convolutional.ZeroPadding2D at 0x7f4d5428eb00>,\n <keras.layers.pooling.MaxPooling2D at 0x7f4d5428e4e0>,\n <keras.layers.convolutional.Conv2D at 0x7f4d54280390>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d4c426630>,\n <keras.layers.core.Activation at 0x7f4d4c426fd0>,\n <keras.layers.convolutional.Conv2D at 0x7f4d4c3c4cf8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d4c375b70>,\n <keras.layers.core.Activation at 0x7f4d4c3423c8>,\n <keras.layers.convolutional.Conv2D at 0x7f4d4c251ef0>,\n <keras.layers.convolutional.Conv2D at 0x7f4d4c1fb320>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d4c22ed30>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d4c0eab00>,\n <keras.layers.merge.Add at 0x7f4d4c0b44e0>,\n <keras.layers.core.Activation at 0x7f4d2c7da080>,\n <keras.layers.convolutional.Conv2D at 0x7f4d2c7da780>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d2c742668>,\n <keras.layers.core.Activation at 0x7f4d2c6bfbe0>,\n <keras.layers.convolutional.Conv2D at 0x7f4d2c6925f8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d2c621160>,\n <keras.layers.core.Activation at 0x7f4d2c631b70>,\n <keras.layers.convolutional.Conv2D at 0x7f4d2c54f7b8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d2c5335f8>,\n <keras.layers.merge.Add at 0x7f4d2c50a550>,\n <keras.layers.core.Activation at 0x7f4d2c4058d0>,\n <keras.layers.convolutional.Conv2D at 0x7f4d2c4288d0>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d2c385668>,\n <keras.layers.core.Activation at 0x7f4d2c3649b0>,\n <keras.layers.convolutional.Conv2D at 0x7f4d2c296c88>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d2c2f0b70>,\n <keras.layers.core.Activation at 0x7f4d2c23cc18>,\n <keras.layers.convolutional.Conv2D at 0x7f4d2c14a978>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d2c1a7e48>,\n <keras.layers.merge.Add at 0x7f4d2c174f28>,\n <keras.layers.core.Activation at 0x7f4d2c048438>,\n <keras.layers.convolutional.Conv2D at 0x7f4d0c7e8358>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d0c7920f0>,\n <keras.layers.core.Activation at 0x7f4d0c766908>,\n <keras.layers.convolutional.Conv2D at 0x7f4d0c6c1ef0>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d0c69b7f0>,\n <keras.layers.core.Activation at 0x7f4d0c640518>,\n <keras.layers.convolutional.Conv2D at 0x7f4d0c592cc0>,\n <keras.layers.convolutional.Conv2D at 0x7f4d0c4fb978>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d0c556ac8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d0c4626a0>,\n <keras.layers.merge.Add at 0x7f4d0c365908>,\n <keras.layers.core.Activation at 0x7f4d0c303048>,\n <keras.layers.convolutional.Conv2D at 0x7f4d0c303518>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d0c2edc50>,\n <keras.layers.core.Activation at 0x7f4d0c2733c8>,\n <keras.layers.convolutional.Conv2D at 0x7f4d0c1bd4a8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d0c19f2b0>,\n <keras.layers.core.Activation at 0x7f4d0c14a128>,\n <keras.layers.convolutional.Conv2D at 0x7f4d0c0da780>,\n <keras.layers.normalization.BatchNormalization at 0x7f4d0c0505c0>,\n <keras.layers.merge.Add at 0x7f4d0c0762e8>,\n <keras.layers.core.Activation at 0x7f4cec769358>,\n <keras.layers.convolutional.Conv2D at 0x7f4cec6d0898>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cec6f4630>,\n <keras.layers.core.Activation at 0x7f4cec652a90>,\n <keras.layers.convolutional.Conv2D at 0x7f4cec5c06d8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cec5deb38>,\n <keras.layers.core.Activation at 0x7f4cec5aabe0>,\n <keras.layers.convolutional.Conv2D at 0x7f4cec4b8f28>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cec498e10>,\n <keras.layers.merge.Add at 0x7f4cec465ef0>,\n <keras.layers.core.Activation at 0x7f4cec3b5400>,\n <keras.layers.convolutional.Conv2D at 0x7f4cec315320>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cec2be0b8>,\n <keras.layers.core.Activation at 0x7f4cec29a8d0>,\n <keras.layers.convolutional.Conv2D at 0x7f4cec26feb8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cec1cc780>,\n <keras.layers.core.Activation at 0x7f4cec1ee4a8>,\n <keras.layers.convolutional.Conv2D at 0x7f4cec086a90>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cec0e0828>,\n <keras.layers.merge.Add at 0x7f4cec0ab9b0>,\n <keras.layers.core.Activation at 0x7f4ccc79acc0>,\n <keras.layers.convolutional.Conv2D at 0x7f4ccc7394a8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4ccc722c18>,\n <keras.layers.core.Activation at 0x7f4ccc6ab358>,\n <keras.layers.convolutional.Conv2D at 0x7f4ccc673780>,\n <keras.layers.normalization.BatchNormalization at 0x7f4ccc60f2b0>,\n <keras.layers.core.Activation at 0x7f4ccc5db630>,\n <keras.layers.convolutional.Conv2D at 0x7f4ccc50f6d8>,\n <keras.layers.convolutional.Conv2D at 0x7f4ccc439208>,\n <keras.layers.normalization.BatchNormalization at 0x7f4ccc4864e0>,\n <keras.layers.normalization.BatchNormalization at 0x7f4ccc37e7b8>,\n <keras.layers.merge.Add at 0x7f4ccc344940>,\n <keras.layers.core.Activation at 0x7f4ccc299cc0>,\n <keras.layers.convolutional.Conv2D at 0x7f4ccc23a4a8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4ccc225c18>,\n <keras.layers.core.Activation at 0x7f4ccc1a0828>,\n <keras.layers.convolutional.Conv2D at 0x7f4ccc178eb8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4ccc0d37b8>,\n <keras.layers.core.Activation at 0x7f4ccc0f7588>,\n <keras.layers.convolutional.Conv2D at 0x7f4cac789c88>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cac7a6828>,\n <keras.layers.merge.Add at 0x7f4cac7749b0>,\n <keras.layers.core.Activation at 0x7f4cac6a2cc0>,\n <keras.layers.convolutional.Conv2D at 0x7f4cac6404a8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cac628c18>,\n <keras.layers.core.Activation at 0x7f4cac5aa898>,\n <keras.layers.convolutional.Conv2D at 0x7f4cac500e80>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cac4da780>,\n <keras.layers.core.Activation at 0x7f4cac4814a8>,\n <keras.layers.convolutional.Conv2D at 0x7f4cac397a58>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cac3ee7f0>,\n <keras.layers.merge.Add at 0x7f4cac33e978>,\n <keras.layers.core.Activation at 0x7f4cac2e97b8>,\n <keras.layers.convolutional.Conv2D at 0x7f4cac2e9fd0>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cac270be0>,\n <keras.layers.core.Activation at 0x7f4cac1ef860>,\n <keras.layers.convolutional.Conv2D at 0x7f4cac13fe48>,\n <keras.layers.normalization.BatchNormalization at 0x7f4cac121748>,\n <keras.layers.core.Activation at 0x7f4cac0ca470>,\n <keras.layers.convolutional.Conv2D at 0x7f4c8c79ca20>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c8c7f47f0>,\n <keras.layers.merge.Add at 0x7f4c8c743940>,\n <keras.layers.core.Activation at 0x7f4c8c6ed7b8>,\n <keras.layers.convolutional.Conv2D at 0x7f4c8c6edda0>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c8c678ba8>,\n <keras.layers.core.Activation at 0x7f4c8c5f6828>,\n <keras.layers.convolutional.Conv2D at 0x7f4c8c54de10>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c8c528710>,\n <keras.layers.core.Activation at 0x7f4c8c4cd438>,\n <keras.layers.convolutional.Conv2D at 0x7f4c8c3e49e8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c8c3bccf8>,\n <keras.layers.merge.Add at 0x7f4c8c388908>,\n <keras.layers.core.Activation at 0x7f4c8c337780>,\n <keras.layers.convolutional.Conv2D at 0x7f4c8c337d68>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c8c23eb38>,\n <keras.layers.core.Activation at 0x7f4c8c1bd7f0>,\n <keras.layers.convolutional.Conv2D at 0x7f4c8c193dd8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c8c16e6d8>,\n <keras.layers.core.Activation at 0x7f4c8c117400>,\n <keras.layers.convolutional.Conv2D at 0x7f4c6c7c02b0>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c6c7e8748>,\n <keras.layers.merge.Add at 0x7f4c6c78e898>,\n <keras.layers.core.Activation at 0x7f4c6c6bd710>,\n <keras.layers.convolutional.Conv2D at 0x7f4c6c6bdf98>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c6c643b38>,\n <keras.layers.core.Activation at 0x7f4c6c5c67b8>,\n <keras.layers.convolutional.Conv2D at 0x7f4c6c579da0>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c6c5766a0>,\n <keras.layers.core.Activation at 0x7f4c6c51e3c8>,\n <keras.layers.convolutional.Conv2D at 0x7f4c6c451fd0>,\n <keras.layers.convolutional.Conv2D at 0x7f4c6c3d86a0>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c6c432978>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c6c2c11d0>,\n <keras.layers.merge.Add at 0x7f4c6c2939b0>,\n <keras.layers.core.Activation at 0x7f4c6c1e62e8>,\n <keras.layers.convolutional.Conv2D at 0x7f4c6c1e6908>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c6c14af98>,\n <keras.layers.core.Activation at 0x7f4c6c0c4cf8>,\n <keras.layers.convolutional.Conv2D at 0x7f4c6c077f28>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c6c0519b0>,\n <keras.layers.core.Activation at 0x7f4c4c7e2b38>,\n <keras.layers.convolutional.Conv2D at 0x7f4c4c6efef0>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c4c6c9cf8>,\n <keras.layers.merge.Add at 0x7f4c4c697da0>,\n <keras.layers.core.Activation at 0x7f4c4c5e92e8>,\n <keras.layers.convolutional.Conv2D at 0x7f4c4c5e9908>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c4c552f98>,\n <keras.layers.core.Activation at 0x7f4c4c4cbcf8>,\n <keras.layers.convolutional.Conv2D at 0x7f4c4c3ffda0>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c4c45a978>,\n <keras.layers.core.Activation at 0x7f4c4c42c3c8>,\n <keras.layers.convolutional.Conv2D at 0x7f4c4c334eb8>,\n <keras.layers.normalization.BatchNormalization at 0x7f4c4c311cc0>,\n <keras.layers.merge.Add at 0x7f4c4c2deda0>,\n <keras.layers.core.Activation at 0x7f4c4c232278>,\n <keras.layers.pooling.GlobalAveragePooling2D at 0x7f4c4c2328d0>]"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.layers[0].layers"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following."
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.layers[0].trainable = False"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer."
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nresnet50 (Model)             (None, 2048)              23587712  \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 4098      \n=================================================================\nTotal params: 23,591,810\nTrainable params: 4,098\nNon-trainable params: 23,587,712\n_________________________________________________________________\n"
                }
            ],
            "source": "model.summary()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Next we compile our model using the **adam** optimizer."
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "steps_per_epoch_training = len(train_generator)\nsteps_per_epoch_validation = len(validation_generator)\nnum_epochs = 1"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/1\n226/301 [=====================>........] - ETA: 39:20 - loss: 0.0534 - acc: 0.9816"
                }
            ],
            "source": "fit_history = model.fit_generator(\n    train_generator,\n    steps_per_epoch=steps_per_epoch_training,\n    epochs=num_epochs,\n    validation_data=validation_generator,\n    validation_steps=steps_per_epoch_validation,\n    verbose=1,\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Now that the model is trained, you are ready to start using it to classify images."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.save('classifier_resnet_model.h5')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# START CODE BLOCK\n# cos2file - takes an object from Cloud Object Storage and writes it to file on container file system.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: project object defined in project token\n# data_path: the directory to write the file\n# filename: name of the file in COS\n\nimport os\ndef cos2file(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n    open( data_dir + '/' + filename, 'wb').write(p.get_file(filename).read())\n\n# file2cos - takes file on container file system and writes it to an object in Cloud Object Storage.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: prooject object defined in project token\n# data_path: the directory to read the file from\n# filename: name of the file on container file system\n\nimport os\ndef file2cos(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    path_to_file = data_dir + '/' + filename\n    if os.path.exists(path_to_file):\n        file_object = open(path_to_file, 'rb')\n        p.save_data(filename, file_object, set_project_asset=True, overwrite=True)\n    else:\n        print(\"file2cos error: File not found\")\n# END CODE BLOCK"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "file2cos(project, '/work', 'classifier_resnet_model.h5')"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "### Thank you for completing this lab!\n\nThis notebook was created by Alex Aklson. I hope you found this lab interesting and educational."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1)."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<hr>\n\nCopyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}