{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# Logging"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Like all problems in data science, performance monitoring starts with collecting the right data in the right format. Because performance monitoring is a concern in nearly all customer-facing computer systems, there is a well-established set of tools and techniques for collecting this data. Data performance monitoting is generally collected using log files. Recall the following best practice:\n\n**Important**\n* Ensure that your data are collected at the most granular level possible. This means each data point should represent one user making one action or one event.\n\nNaturally, collecting very granular data will result in very large data sets. If there is a need, you can always summarize the data after it has been collected. Summary level data may mask important patterns and generally it is not possible to go from summary data to granular data. Log files with several million entries can be analyzed on a single node or on a personal machine with little overhead.\n\n**Note: If the logging is handled by another member of your team or by another team you should ensure that the minimally required data discussed here are available or it will be difficult to monitor your model's performance and/or debug performance related issues.**"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Minimal requirements for log files"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "These are data that are minimally required for performance monitoring for most model deployment projects. There are other features that fall into this category that are situation dependent, like user_id in a recommendation system, so do not view this list as comprehensive, simply keep it as a reference starting point.\n\n**runtime** - The total amount of time required to process the request. This is a factor that directly affects the end user\u2019s experience and should be monitored.\n\n**timestamp** - Timestamps are needed to evaluate how well the system handles load and concurrency. Additionally, timestamps are useful when connecting predictions to labels that are acquired afterwards. Finally, they are needed for the investigation of events that might affect the relationship between the performance and business metrics.\n\n**prediction** - The prediction is, of course, the primary output of a prediction model. It is necessary to track the prediction for comparison to feedback to determine the quality of the predictions. Generally, predictions are returned as a list to accommodate multi-label classification.\n\n**input_data_summary** - Summarizing information about the input data itself. For the predict endpoint this is the shape of the input feature matrix, but for the training endpoint the features and targets should be summarized.\n\n**model_version_number** - The model version number is used to better understand the influence of model improvements (or bugs) on performance.\n\n### Additional features that can be optionally logged\n\nThese are the features that are considered nice to have, but they are not always relevant to the circumstances or sometimes there can be practical limitations (e.g. disk space or computational resources) that limit the ability to save these features.\n\n**request_unique_id** - Each request that has been made should correspond to an entry in the log file. It is possible that a request corresponds to more than one entry in the log file for example if more than one model is called. This is also known as correlation_id.\n\n**data** - Saving the input features that were provided at the time of a predict request makes it much easier to debug broken requests. Saving the features and target at the time of training makes it easier to debug broken model training.\n\n**request_type** - Relevant attributes about the request (e.g. webapp request, browser request)\n\n**probability** - Probability associated with a prediction (if applicable)\n\n\nThe value of logging most of the mentioned data is fairly intuitive, but saving the data itself might seem unnecessary. If we save the input features, when a predict endpoint was hit, we can reconstruct the individual prediction, stepping through each part of the prediction process. For training, the archiving of all the data is often unnecessary, because there is a system in place, like a centralized database, that can re-create the training data for a given point in time. One option is to archive only the previous iteration of training data."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Logging in python"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Python has a logging module which can be used for performance monitoring, but we will show logging through the use of the csv module to keep the process as simple as possible. The following code shows how to create a log file."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "#!pip install joblib"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Overwriting example-logging.py\n"
                }
            ],
            "source": "%%writefile example-logging.py\n\n#!/usr/bin/env python\n\"\"\"\nuse the iris data to demonstrate how logging is tied to \na machine learning model to enable performance monitoring\n\"\"\"\n\nimport time,os,re,csv,sys,uuid,joblib\nfrom datetime import date\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\ndef train_model(X,y,saved_model):\n    \"\"\"\n    function to train model\n    \"\"\"\n\n    ## Perform a train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n    ## Specify parameters and model\n    params = {'C':1.0,'kernel':'linear','gamma':0.5}\n    clf = svm.SVC(**params,probability=True)\n\n    ## fit model on training data\n    clf = clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(classification_report(y_test,y_pred))\n\n    ## retrain using all data\n    clf.fit(X, y)\n    print(\"... saving model: {}\".format(saved_model))\n    joblib.dump(clf,saved_model)\n\n    \ndef _update_predict_log(y_pred,y_proba,query,runtime):\n    \"\"\"\n    update predict log file\n    \"\"\"\n\n    ## name the logfile using something that cycles with date (day, month, year)    \n    today = date.today()\n    logfile = \"example-predict-{}-{}.log\".format(today.year, today.month)\n\n    ## write the data to a csv file    \n    header = ['unique_id','timestamp','y_pred','y_proba','x_shape','model_version','runtime']\n    write_header = False\n    if not os.path.exists(logfile):\n        write_header = True\n    with open(logfile,'a') as csvfile:\n        writer = csv.writer(csvfile, delimiter=',', quotechar='|')\n        if write_header:\n            writer.writerow(header)\n\n        to_write = map(str,[uuid.uuid4(),time.time(),y_pred,y_proba,query.shape,MODEL_VERSION,runtime])\n        writer.writerow(to_write)\n\ndef predict(query):\n    \"\"\"\n    generic function for prediction\n    \"\"\"\n\n    ## start timer for runtime\n    time_start = time.time()\n    \n    ## ensure the model is loaded\n    model = joblib.load(saved_model)\n\n    ## output checking\n    if len(query.shape) == 1:\n        query = query.reshape(1, -1)\n    \n    ## make prediction and gather data for log entry\n    y_pred = model.predict(query)\n    y_proba = None\n    if 'predict_proba' in dir(model) and model.probability == True:\n        y_proba = model.predict_proba(query)\n    m, s = divmod(time.time()-time_start, 60)\n    h, m = divmod(m, 60)\n    runtime = \"%03d:%02d:%02d\"%(h, m, s)\n\n    ## update the log file\n    _update_predict_log(y_pred,y_proba,query,runtime)\n    \n    return(y_pred)\n\nif __name__ == \"__main__\":\n\n    ## import some data to play with\n    iris = datasets.load_iris()\n    X = iris.data[:,:2]\n    y = iris.target\n\n    ## train the model\n    MODEL_VERSION = 1.0\n    saved_model = \"example-predict-{}.joblib\".format(re.sub(\"\\.\",\"_\",str(MODEL_VERSION)))\n    model = train_model(X,y,saved_model)\n\n    ## example predict\n    for query in [np.array([[6.1,2.8]]), np.array([[7.7,2.5]]), np.array([[5.8,3.8]])]:\n        y_pred = predict(query)\n        print(\"predicted: {}\".format(y_pred))\n"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        19\n           1       0.67      0.53      0.59        15\n           2       0.63      0.75      0.69        16\n\n   micro avg       0.78      0.78      0.78        50\n   macro avg       0.77      0.76      0.76        50\nweighted avg       0.78      0.78      0.78        50\n\n... saving model: example-predict-1_0.joblib\npredicted: [1]\npredicted: [2]\npredicted: [0]\n"
                }
            ],
            "source": "!python example-logging.py"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "unique_id,timestamp,y_pred,y_proba,x_shape,model_version,runtime\n38f9d8a9-f508-4377-aaa5-d0731b806f0d,1588078438.7247682,[1],[[0.00655896 0.55274448 0.44069655]],|(1, 2)|,1.0,000:00:00\n9e655f8d-8173-4a2b-802e-ea5b7c9cde74,1588078438.7315822,[2],[[3.78813625e-05 1.70410037e-01 8.29552082e-01]],|(1, 2)|,1.0,000:00:00\n047d2ea0-843c-4ec5-b637-78d057993c25,1588078438.7370567,[0],[[0.71067191 0.13553971 0.15378837]],|(1, 2)|,1.0,000:00:00\ndf31294f-a806-45b1-81db-191c39630598,1588078805.3078685,[1],[[0.00605679 0.58266977 0.41127344]],|(1, 2)|,1.0,000:00:00\naf437e22-6fb9-4c99-ba68-38a4d0188dc6,1588078805.3134978,[2],[[2.10800331e-05 1.31788410e-01 8.68190510e-01]],|(1, 2)|,1.0,000:00:00\ncdb88097-6fe9-4532-b2c9-f51d341b0d36,1588078805.3194745,[0],[[0.72022465 0.13827636 0.14149899]],|(1, 2)|,1.0,000:00:00\n\n"
                }
            ],
            "source": "logs = open(\"example-predict-2020-4.log\", \"r\").read()\nprint(logs)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "It is reasonable to use JSON or a centralized database as a target destination. There are numerous other tools like Elasticsearch and Apache Commons Logging. We use simple CSV formatted files because they help keep the Docker container isolated from other environments and they are a convenient format for most data scientists."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}