{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# Introduction"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In terms of a business opportunity we can consider the use of a recommender system when any of the following questions are relevant:\n\n* What would a user like?\n* What would a user buy?\n* What would a user click?\n\n> *Is there something we could suggest to improve a user experience?*\n\nThere are other situations where recommendations might be appropriate outside of the scope of these questions. One example would be if the AAVAIL team wanted to recommend words or phrases for an autofill feature that is part of the company's website or app. To consider a recommender system, we need appropriate data. This most often comes in the form of ratings matrix, sometimes known as utility matrix. Here is what a piece of ratings matrix might look like for AAVAIL data.\n\n|User|Feed 1|Feed 2|Feed 3|Feed 4|Feed 5|Feed 6|Feed 7|Feed 8|Feed 9|\n|:--:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n|1 | ? | ? | 4 | ? | ? | ? | 1 | ? | ? |\n|2 | ? | ? | 4 | ? | ? | ? | 1 | ? | 2 |\n|3 | ? | ? | 4 | 5 | ? | 1 | 1 | 3 | ? |\n|4 | 3 | 2 | ? | ? | ? | 1 | ? | ? | ? |\n|5 | 1 | 4 | 4 | ? | ? | 1 | 1 | ? | ? |\n\nNotice that the majority of entries are missing, as it typically wiht utility matrices. We can't expect that every user has watched every feed, or even a significant portion of them. Most User/Feed intersections will be unrated, or blank, resulting in a sparse matrix.\n\nRatings come in two types: **explicit** and **implicit**. The above utility matrix contains explicit ratings because the users rated feeds directly. Implicit data is derived from a user's behaviors or actions for example likes, shares, page visits or amount of time watched. These can be used to construct a utility matrix. Keeping with our AAVAIL feed example, we can engineer a measure based on *indirect* evidence. For example the score for **Feed 1** could be based on a user's location, comment history, preferred type of feed, specified topic preferences and more. Each element that contributes to the overall score could have a maximal value of 1.0 and the final number could be scaled to a range of 1-5. Explicit and implicit data can be combined using this type of approach as well and naturally you would want to have a solid understanding of the stories before engineering a score.\n\nMost recommender systems today are able to leverage both explicit (e.g. numerical ratings) and implicit (e.g. likes, purchases, skipped, bookmarked) patterns in a ratings matrix. The SVD++ algorithm is an example of a method that exploits both patterns."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Recommendation Systems"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The majority of modern recommender systems embrace either a collaborative filtering or a content-based approach. A number of other approaches and hybrids exist making some implemented systems difficult to categorize.\n\n> **Collaborative filtering:** Collaborative filtering is a family of methods that identify a subset of users who have preferences similar to the target user. From these, a ratings matrix is created. The items preferred by these users are combined and filtered to create a ranked list of recommended items. Recommendations based on similarity to infer preference or behavior.\n\n> **Content-based filtering:** Predictions are made based on the properties and characteristics of an item. User behavior is not considered.\n\n> **Hybrid recommender systems:** A combination of collaborative filtering and content based systems.\n\n> **Other types of systems:** Some systems especially legacy ones were based on demographics. Other systems attempt to infer utility before making a recommendation.\n\n### Matrix factorization techniques\n\n> *Find the latent factors that help explain the patterns in a ratings matrix*\n\n*Matrix factorization* is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix to the product of lower-dimension matrices. In general, the user-item interaction matrix will be very, very large, and very sparse. The lower-dimension matrices will be much smaller and denser and can be used to reconstruct the user-item interaction matrix, including predictions where values were previously missing.\n\n**Common approaches**\n\n* Singular Value Decomposition (SVD)\n* UV Decomponsition (UVD)\n* non-negative matrix factorization (NMF)\n\nMatrix factorization is generally accomplished using *Alternating Least Squares (ALS)* or *Stochastic Gradient Descent (SGD)*. Hyperparameters are used to control the regularization and the relative weighting of implicit versus explicit rating matrices. With recommender systems we are most concerned with scale at prediction. Because user ratings change slowly, if at all, the algorithm does not need to be retrained frequently and so this can be done at night. For this reason, **Spark** is a common platform for developing recommender systems. The computation is already distributed under the Spark framework so scaling infrastructure is straightforward.\n\nThe are several Python packages available to help create recommenders including `surprise`. Because scale with respect to prediction is often a concern for recommender systems, many production environments use the implementation found in Spark MLlib. The Spark collaborative filtering implementation uses Altering least Squares."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Surprise Package"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200425154708-0001\nKERNEL_ID = a32adf45-6cae-459c-8c6d-10b30eae33da\n"
                }
            ],
            "source": "#!pip install surprise"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Dataset ml-100k could not be found. Do you want to download it? [Y/n] Y\nTrying to download dataset from http://files.grouplens.org/datasets/movielens/ml-100k.zip...\nDone! Dataset ml-100k has been saved to /home/spark/shared/.surprise_data/ml-100k\ntest RMSE: 0.936\n"
                }
            ],
            "source": "# Load the movielens-100k dataset\ndata = Dataset.load_builtin(\"ml-100k\")\n\n# We will use the famous SVD algorithm\nalgo = SVD()\n\n# Run 5-fold cross-validation and print the results\nresults = cross_validate(algo, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=False)\n\nprint(\"test RMSE: {}\".format(round(results[\"test_rmse\"].mean(), 3)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Spark MLlib Recommender Example"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "import os\nimport shutil\nimport pyspark as ps\nfrom pyspark.ml import Pipeline, Transformer\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import DoubleType"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "2.4.3\n"
                }
            ],
            "source": "## ensure the spark context is available\nspark = (ps.sql.SparkSession.builder\n         .appName(\"sandbox\")\n         .getOrCreate()\n        )\n\nsc = spark.sparkContext\nprint(spark.version)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Make Notebook Run within IBM Watson"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "# START CODE BLOCK\n# cos2file - takes an object from Cloud Object Storage and writes it to file on container file system.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: project object defined in project token\n# data_path: the directory to write the file\n# filename: name of the file in COS\n\nimport os\ndef cos2file(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n    open( data_dir + '/' + filename, 'wb').write(p.get_file(filename).read())\n\n# file2cos - takes file on container file system and writes it to an object in Cloud Object Storage.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: prooject object defined in project token\n# data_path: the directory to read the file from\n# filename: name of the file on container file system\n\nimport os\ndef file2cos(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    path_to_file = data_dir + '/' + filename\n    if os.path.exists(path_to_file):\n        file_object = open(path_to_file, 'rb')\n        p.save_data(filename, file_object, set_project_asset=True, overwrite=True)\n    else:\n        print(\"file2cos error: File not found\")\n# END CODE BLOCK"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "cos2file(project, '/data', 'movies.csv')\ncos2file(project, '/data', 'ratings.csv')"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "# download the sample movie lens ratings\ndata_dir = os.path.join(\".\", \"data\")\nratings_file = os.path.join(data_dir, \"ratings.csv\")\nmovies_file = os.path.join(data_dir, \"movies.csv\")"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------+-------+------+---------+\n|userId|movieId|rating|timestamp|\n+------+-------+------+---------+\n|     1|      1|   4.0|964982703|\n|     1|      3|   4.0|964981247|\n|     1|      6|   4.0|964982224|\n|     1|     47|   5.0|964983815|\n+------+-------+------+---------+\nonly showing top 4 rows\n\n+-------+--------------------+--------------------+\n|movieId|               title|              genres|\n+-------+--------------------+--------------------+\n|      1|    Toy Story (1995)|Adventure|Animati...|\n|      2|      Jumanji (1995)|Adventure|Childre...|\n|      3|Grumpier Old Men ...|      Comedy|Romance|\n|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n+-------+--------------------+--------------------+\nonly showing top 4 rows\n\n"
                }
            ],
            "source": "# Load the data\ndf = spark.read.csv(ratings_file, header=True, inferSchema=True)\ndf2 = spark.read.csv(movies_file, header=True, inferSchema=True)\n\ndf.show(n=4)\ndf2.show(n=4)\n\n# split the data to training and test sets\n(training, test) = df.randomSplit([0.8, 0.2])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model Training"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "RMSE = 1.093\n"
                }
            ],
            "source": "## train the recommender model with ALS\nals_alg = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n              coldStartStrategy=\"drop\")\n\nmodel = als_alg.fit(training)\n\n## evaluate the recommender with the holdout set\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n\nrmse = evaluator.evaluate(predictions)\nprint(\"RMSE = {}\".format(round(rmse, 3)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Generate user and movie recommendations"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------+--------------------+\n|userId|     recommendations|\n+------+--------------------+\n|   471|[[7099, 8.933234]...|\n|   463|[[175303, 7.42290...|\n|   496|[[86320, 8.286928...|\n|   148|[[3099, 6.1729946...|\n+------+--------------------+\nonly showing top 4 rows\n\n"
                }
            ],
            "source": "## Generate top 10 movie recommendations for each user\nuser_recs = model.recommendForAllUsers(10)\nuser_recs.show(n=4)"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+-------+--------------------+\n|movieId|     recommendations|\n+-------+--------------------+\n|   1580|[[37, 5.566883], ...|\n|   4900|[[549, 11.236831]...|\n|   5300|[[296, 8.064962],...|\n|   6620|[[55, 6.7833996],...|\n+-------+--------------------+\nonly showing top 4 rows\n\n"
                }
            ],
            "source": "## Generate top 10 user recommendations for each movie\nmovies_recs = model.recommendForAllItems(10)\nmovies_recs.show(n=4)"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------+--------------------+\n|userId|     recommendations|\n+------+--------------------+\n|   471|[[7099, 8.933234]...|\n|   463|[[175303, 7.42290...|\n|   148|[[3099, 6.1729946...|\n+------+--------------------+\n\n"
                }
            ],
            "source": "## Generate top 10 movie recommendations for a specified set of users\nusers = df.select(als_alg.getUserCol()).distinct().limit(3)\nusers_subset_recs = model.recommendForUserSubset(users, 10)\nusers_subset_recs.show(4)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+-------+--------------------+\n|movieId|     recommendations|\n+-------+--------------------+\n|   1580|[[37, 5.566883], ...|\n|   3175|[[296, 7.0045047]...|\n|   2366|[[296, 9.403867],...|\n+-------+--------------------+\n\n"
                }
            ],
            "source": "## Generate top 10 user recommendations for a specified set of movies\nmovies = df.select(als_alg.getItemCol()).distinct().limit(3)\nmovie_subset_recs = model.recommendForItemSubset(movies, 10)\nmovie_subset_recs.show(n=4)"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Maximum Overdrive (1986) --> Horror\nLost in America (1985) --> Comedy\nStealing Home (1988) --> Drama\n"
                }
            ],
            "source": "## match the recs to movie ids\nrecs = movie_subset_recs.toPandas()\nmovies = df2.toPandas()\nrec_titles = movies[\"title\"][recs[\"movieId\"]].tolist()\nrec_genres = movies[\"genres\"][recs[\"movieId\"]].tolist()\n\nfor r, title in enumerate(rec_titles):\n    print(title, \"-->\", rec_genres[r])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model Persistence"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "## remove directoy if already exists\nsave_dir = \"saved-recommender\"\nif os.path.isdir(save_dir):\n    print(\"overwritting saved model\")\n    shutil.rmtree(save_dir)\n    \n# save model\nmodel.save(save_dir)"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "from_saved_model = ALSModel.load(save_dir)"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[Row(userId=1, movieId=2, prediction=3.8280413150787354), Row(userId=2, movieId=10, prediction=3.844179153442383), Row(userId=3, movieId=20, prediction=-2.1697897911071777)]\n"
                }
            ],
            "source": "test = spark.createDataFrame([(1, 2), (2, 10), (3, 20)], [\"userId\", \"movieId\"])\npredictions = sorted(model.transform(test).collect(), key=lambda r: r[0])\nprint(predictions)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Recommendations Systems in Production"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Cold Start"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "One issue that arises with recommender systems in production is known as the cold-start problem. Cold start is a potential problem in computer-based information systems which involve a degree of automated data modelling. Specifically, it concerns the issue that the system cannot draw any inferences for users or items about which it has not yet gathered sufficient information.\n\nThere are two scenarios when it comes to the cold start problem:\n\n**What shall we recommend to a new used?**\n* If the recommender is popularity-based then the most popular items are recommended and this is not a problem. If the recommender is similarity-based, the user could rate five items as part of the sign-up or you could attempt to infer similarity based on user meta-data such as age, gender, location, etc. Even if recommendations are based on similarities, you may still use the most popular items to get the user started, but you would likely want to customize the list possibly based on meta-data.\n\n**How should we treat a new item that hasn't been reviewed?**\n* In order to make good recommendations, you need data about how users review the item. But until the item has been recommended, it's unlikely that users will review it. To overcome this dilemma, the item could be randomly suggested for a trial period to collect data. You could put it in a special section such as a new releases to gauge initial request. You can also use meta-data associated with the item to find similar items and infer its recommendations from these similar items."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Concurrency** can be a challenge for recommenders systems. A recommender might, for example, find the 20 closest users based on latent factor profiles. From those users it would identify a list of potential recommendations that could be sorted and filtered given what is known about the user. The distances between users can often be pre-computed to speed up the recommendations because user characteristics change slowly. Nevertheless, this process has a few steps to it that require a burst of compute time. If five users hit the service at the same time, there is possibility that the processors get weighted down with these simultaneous requests and recommendations become u"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}