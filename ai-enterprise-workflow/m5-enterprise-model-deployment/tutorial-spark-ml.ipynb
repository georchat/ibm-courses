{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Make Notebook run with IBM Watson"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200423112957-0004\nKERNEL_ID = 7a15a6c5-cf58-4e64-9128-97aa81491851\n"
                }
            ],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# START CODE BLOCK\n# cos2file - takes an object from Cloud Object Storage and writes it to file on container file system.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: project object defined in project token\n# data_path: the directory to write the file\n# filename: name of the file in COS\n\nimport os\ndef cos2file(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n    open( data_dir + '/' + filename, 'wb').write(p.get_file(filename).read())\n\n# file2cos - takes file on container file system and writes it to an object in Cloud Object Storage.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: prooject object defined in project token\n# data_path: the directory to read the file from\n# filename: name of the file on container file system\n\nimport os\ndef file2cos(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    path_to_file = data_dir + '/' + filename\n    if os.path.exists(path_to_file):\n        file_object = open(path_to_file, 'rb')\n        p.save_data(filename, file_object, set_project_asset=True, overwrite=True)\n    else:\n        print(\"file2cos error: File not found\")\n# END CODE BLOCK"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "cos2file(project, '/data', 'aavail-target.csv')"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "-"
                }
            },
            "source": "# Spark Machine Learning\n\nKeep the main [Spark ML documentation](https://spark.apache.org/docs/latest/ml-pipeline.html) as you go through this tutorial.  MLlib is Spark\u2019s machine learning (ML) library. **Spark ML** is not an official name, but we will use it to refer to the MLlib DataFrame-based API that embraces ML pipelines. Before we get into Spark ML by demonstrating a couple of examples we will first review Spark DataFrames."
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": "import re\nimport os\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn')\n%matplotlib inline"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": "## Spark SQL and DataFrames\n\nWhat is Spark SQL?\n- Spark SQL takes basic RDDs and **puts a schema on them**.\n\nWhat is a DataFrame?\n- DataFrames are the primary abstraction in Spark SQL.\n- Think of a DataFrames as **RDDs with schema**.\n\nWhat are **schemas**?\n- Schemas are metadata about your data.\n- Schema = Table Names + Column Names + Column Types\n\nWhat are the pros of schemas?\n- Schemas enable using **column names** instead of column positions\n- Schemas enable **queries** using SQL and DataFrame syntax\n- Schemas make your data more **structured**.\n\nSee the [Spark SQL documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html) as a main point of reference for Spark SQL, DataFrames and Datasets."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": "## Creating DataFrames\n\nYou can create a DataFrame from an existing RDD (whatever source you used to create this one), if you add a schema.\n\nTo build a schema, you will use existing data types provided in the [`pyspqrk.sql.types`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) module.  \n\n<center>\n<table style=\"width:50%\">\n  <tr>\n      <th>Types</th>\n      <th>Python Equivalent</th>\n    </tr>\n  <tr>\n      <td>StringType</td>\n      <td>string</td>\n  </tr>\n  <tr>\n      <td>IntegerType</td>\n      <td>integer</td>\n   <tr>\n      <td>FloatType</td>\n      <td>float</td> \n  <tr>\n      <td>ArrayType</td>\n      <td>array or list</td>\n   </tr>\n    <tr>\n      <td>MapType</td>\n      <td>dict</td>\n   </tr>       \n</table>\n</center>\n\nFirst we initialize the Spark Environment"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "import pyspark as ps\n\nspark = ps.sql.SparkSession.builder \\\n            .master(\"local[4]\") \\\n            .appName(\"spark-ml-examples\") \\\n            .getOrCreate()\n\nsc = spark.sparkContext"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "the `local[4]` will create a `local` cluster made of the driver using all 4 cores.  Lets start with a very small file to to demonstrate the different ways to create Spark DataFrames."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[(1, 1, 'united_states', 21, 'Kasen Todd', 'aavail_premium', 23),\n (2, 0, 'singapore', 30, 'Ensley Garza', 'aavail_unlimited', 12),\n (3, 0, 'united_states', 21, 'Lillian Carey', 'aavail_premium', 22),\n (4, 1, 'united_states', 20, 'Beau Christensen', 'aavail_basic', 19),\n (5, 1, 'singapore', 21, 'Ernesto Gibson', 'aavail_premium', 23)]"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "data_dir = os.path.join(\".\",\"data\")\ndef casting_function(args):\n    customer_id, is_subscriber, country, age, customer_name, subscriber_type, num_streams = args\n    return((int(customer_id), int(is_subscriber), country, int(age), customer_name, subscriber_type, int(num_streams)))\n\nrdd_aavail = sc.textFile(os.path.join(data_dir,'aavail-target.csv'))\\\n                         .map(lambda rowstr : rowstr.split(\",\"))\\\n                         .filter(lambda row: not row[0].startswith('c'))\\\n                         .map(casting_function)\n\nrdd_aavail.collect()[:5]"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "notes"
                }
            },
            "source": "You can create a Spark DataFrame using a schema that you have defined or it can be inferred.  To create your own. "
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+-----------+-------------+-------------+---+----------------+----------------+-----------+\n|customer_id|is_subscriber|      country|age|   customer_name| subscriber_type|num_streams|\n+-----------+-------------+-------------+---+----------------+----------------+-----------+\n|          1|            1|united_states| 21|      Kasen Todd|  aavail_premium|         23|\n|          2|            0|    singapore| 30|    Ensley Garza|aavail_unlimited|         12|\n|          3|            0|united_states| 21|   Lillian Carey|  aavail_premium|         22|\n|          4|            1|united_states| 20|Beau Christensen|    aavail_basic|         19|\n|          5|            1|    singapore| 21|  Ernesto Gibson|  aavail_premium|         23|\n|          6|            1|united_states| 21|  Deshawn Murray|  aavail_premium|         20|\n|          7|            0|    singapore| 48|     Daxton Tate|    aavail_basic|         18|\n|          8|            1|united_states| 47|    Tenley Small|  aavail_premium|         20|\n|          9|            0|united_states| 21|      Kyra Chase|  aavail_premium|         24|\n|         10|            0|united_states| 26|   London Barber|    aavail_basic|         20|\n|         11|            1|united_states| 14|     Rohan Rivas|  aavail_premium|         18|\n|         12|            0|    singapore| 39|     Romina Ross|aavail_unlimited|          1|\n|         13|            1|united_states| 20|   Liberty Moore|  aavail_premium|         21|\n|         14|            1|united_states| 19|     Yusuf Huber|    aavail_basic|         22|\n|         15|            0|united_states| 22| Brantley Suarez|  aavail_premium|         20|\n|         16|            1|united_states| 22|      Skyler Ray|    aavail_basic|          5|\n|         17|            1|united_states| 27|       Joel Boyd|  aavail_premium|         24|\n|         18|            0|    singapore| 44|    Zayne Torres|    aavail_basic|         17|\n|         19|            1|united_states| 28|    Madden Cross|  aavail_premium|         19|\n|         20|            1|united_states| 20|    Jabari Eaton|  aavail_premium|          5|\n+-----------+-------------+-------------+---+----------------+----------------+-----------+\nonly showing top 20 rows\n\nroot\n |-- customer_id: integer (nullable = true)\n |-- is_subscriber: integer (nullable = true)\n |-- country: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- customer_name: string (nullable = true)\n |-- subscriber_type: string (nullable = true)\n |-- num_streams: integer (nullable = true)\n\n"
                }
            ],
            "source": "from pyspark.sql.types import *\n\nschema = StructType([\n    StructField('customer_id',IntegerType(),True),\n    StructField('is_subscriber',IntegerType(),True),\n    StructField('country',StringType(),True),\n    StructField('age',IntegerType(),True),\n    StructField('customer_name',StringType(),True),\n    StructField('subscriber_type',StringType(),True),\n    StructField('num_streams',IntegerType(),True) ])\n    \n# feed that into a DataFrame\ndf = spark.createDataFrame(rdd_aavail,schema)\n\n# show the result\ndf.show()\n\n# print the schema\ndf.printSchema()  "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "You may also read the data directly from a file and **infer** the schema"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "slideshow": {
                    "slide_type": "notes"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "root\n |-- customer_id: integer (nullable = true)\n |-- is_subscriber: integer (nullable = true)\n |-- country: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- customer_name: string (nullable = true)\n |-- subscriber_type: string (nullable = true)\n |-- num_streams: integer (nullable = true)\n\nline count: 1000\n+-----------+-------------+-------------+---+----------------+----------------+-----------+\n|customer_id|is_subscriber|      country|age|   customer_name| subscriber_type|num_streams|\n+-----------+-------------+-------------+---+----------------+----------------+-----------+\n|          1|            1|united_states| 21|      Kasen Todd|  aavail_premium|         23|\n|          2|            0|    singapore| 30|    Ensley Garza|aavail_unlimited|         12|\n|          3|            0|united_states| 21|   Lillian Carey|  aavail_premium|         22|\n|          4|            1|united_states| 20|Beau Christensen|    aavail_basic|         19|\n|          5|            1|    singapore| 21|  Ernesto Gibson|  aavail_premium|         23|\n|          6|            1|united_states| 21|  Deshawn Murray|  aavail_premium|         20|\n|          7|            0|    singapore| 48|     Daxton Tate|    aavail_basic|         18|\n|          8|            1|united_states| 47|    Tenley Small|  aavail_premium|         20|\n|          9|            0|united_states| 21|      Kyra Chase|  aavail_premium|         24|\n|         10|            0|united_states| 26|   London Barber|    aavail_basic|         20|\n|         11|            1|united_states| 14|     Rohan Rivas|  aavail_premium|         18|\n|         12|            0|    singapore| 39|     Romina Ross|aavail_unlimited|          1|\n|         13|            1|united_states| 20|   Liberty Moore|  aavail_premium|         21|\n|         14|            1|united_states| 19|     Yusuf Huber|    aavail_basic|         22|\n|         15|            0|united_states| 22| Brantley Suarez|  aavail_premium|         20|\n|         16|            1|united_states| 22|      Skyler Ray|    aavail_basic|          5|\n|         17|            1|united_states| 27|       Joel Boyd|  aavail_premium|         24|\n|         18|            0|    singapore| 44|    Zayne Torres|    aavail_basic|         17|\n|         19|            1|united_states| 28|    Madden Cross|  aavail_premium|         19|\n|         20|            1|united_states| 20|    Jabari Eaton|  aavail_premium|          5|\n+-----------+-------------+-------------+---+----------------+----------------+-----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# read CSV\ndf = spark.read.csv(os.path.join(data_dir,'aavail-target.csv'),\n                         header=True,       # use headers or not\n                         quote='\"',         # char for quotes\n                         sep=\",\",           # char for separation\n                         inferSchema=True)  # do we infer schema or not ?\n\n# prints the schema\ndf.printSchema()\n\n# some functions are still valid\nprint(\"line count: {}\".format(df.count()))\n\n# show the table in a nice format\ndf.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "You can turn the DataFrame into a Panda DataFrame, but be careful since this 'action' will put all the data into memory"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>is_subscriber</th>\n      <th>country</th>\n      <th>age</th>\n      <th>customer_name</th>\n      <th>subscriber_type</th>\n      <th>num_streams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>21</td>\n      <td>Kasen Todd</td>\n      <td>aavail_premium</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>singapore</td>\n      <td>30</td>\n      <td>Ensley Garza</td>\n      <td>aavail_unlimited</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>united_states</td>\n      <td>21</td>\n      <td>Lillian Carey</td>\n      <td>aavail_premium</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>20</td>\n      <td>Beau Christensen</td>\n      <td>aavail_basic</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>singapore</td>\n      <td>21</td>\n      <td>Ernesto Gibson</td>\n      <td>aavail_premium</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>21</td>\n      <td>Deshawn Murray</td>\n      <td>aavail_premium</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0</td>\n      <td>singapore</td>\n      <td>48</td>\n      <td>Daxton Tate</td>\n      <td>aavail_basic</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>47</td>\n      <td>Tenley Small</td>\n      <td>aavail_premium</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0</td>\n      <td>united_states</td>\n      <td>21</td>\n      <td>Kyra Chase</td>\n      <td>aavail_premium</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0</td>\n      <td>united_states</td>\n      <td>26</td>\n      <td>London Barber</td>\n      <td>aavail_basic</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>14</td>\n      <td>Rohan Rivas</td>\n      <td>aavail_premium</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>0</td>\n      <td>singapore</td>\n      <td>39</td>\n      <td>Romina Ross</td>\n      <td>aavail_unlimited</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>20</td>\n      <td>Liberty Moore</td>\n      <td>aavail_premium</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>19</td>\n      <td>Yusuf Huber</td>\n      <td>aavail_basic</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>0</td>\n      <td>united_states</td>\n      <td>22</td>\n      <td>Brantley Suarez</td>\n      <td>aavail_premium</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>22</td>\n      <td>Skyler Ray</td>\n      <td>aavail_basic</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>27</td>\n      <td>Joel Boyd</td>\n      <td>aavail_premium</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>18</td>\n      <td>0</td>\n      <td>singapore</td>\n      <td>44</td>\n      <td>Zayne Torres</td>\n      <td>aavail_basic</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>28</td>\n      <td>Madden Cross</td>\n      <td>aavail_premium</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>20</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>20</td>\n      <td>Jabari Eaton</td>\n      <td>aavail_premium</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>21</td>\n      <td>0</td>\n      <td>singapore</td>\n      <td>38</td>\n      <td>Claudia Hickman</td>\n      <td>aavail_unlimited</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>22</td>\n      <td>0</td>\n      <td>singapore</td>\n      <td>41</td>\n      <td>Darwin Parker</td>\n      <td>aavail_basic</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>23</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>22</td>\n      <td>Josiah Pruitt</td>\n      <td>aavail_unlimited</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>24</td>\n      <td>0</td>\n      <td>singapore</td>\n      <td>23</td>\n      <td>Derek Landry</td>\n      <td>aavail_basic</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>25</td>\n      <td>1</td>\n      <td>singapore</td>\n      <td>20</td>\n      <td>Sage Dejesus</td>\n      <td>aavail_basic</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>26</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>24</td>\n      <td>Mikaela Adams</td>\n      <td>aavail_premium</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>27</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>21</td>\n      <td>Jimmy Lee</td>\n      <td>aavail_basic</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>28</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>18</td>\n      <td>Devin Hubbard</td>\n      <td>aavail_basic</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>29</td>\n      <td>1</td>\n      <td>singapore</td>\n      <td>20</td>\n      <td>Kameron Miles</td>\n      <td>aavail_basic</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>30</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>39</td>\n      <td>Francisco Jennings</td>\n      <td>aavail_unlimited</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>970</th>\n      <td>971</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>26</td>\n      <td>Teagan Griffin</td>\n      <td>aavail_premium</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>971</th>\n      <td>972</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>19</td>\n      <td>Diana Wise</td>\n      <td>aavail_unlimited</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>972</th>\n      <td>973</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>44</td>\n      <td>Maryam Sheppard</td>\n      <td>aavail_basic</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>973</th>\n      <td>974</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>13</td>\n      <td>Simon Stokes</td>\n      <td>aavail_unlimited</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>974</th>\n      <td>975</td>\n      <td>0</td>\n      <td>united_states</td>\n      <td>-48</td>\n      <td>Leo Harper</td>\n      <td>aavail_basic</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>975</th>\n      <td>976</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>22</td>\n      <td>Thatcher Best</td>\n      <td>aavail_basic</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>976</th>\n      <td>977</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>24</td>\n      <td>Crew West</td>\n      <td>aavail_unlimited</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>977</th>\n      <td>978</td>\n      <td>0</td>\n      <td>singapore</td>\n      <td>19</td>\n      <td>Boston Stark</td>\n      <td>aavail_premium</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>978</th>\n      <td>979</td>\n      <td>1</td>\n      <td>singapore</td>\n      <td>24</td>\n      <td>Elisha Bishop</td>\n      <td>aavail_basic</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>979</th>\n      <td>980</td>\n      <td>0</td>\n      <td>united_states</td>\n      <td>19</td>\n      <td>Forest Bradshaw</td>\n      <td>aavail_premium</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>980</th>\n      <td>981</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>22</td>\n      <td>Carlos Mckenzie</td>\n      <td>aavail_unlimited</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>981</th>\n      <td>982</td>\n      <td>1</td>\n      <td>singapore</td>\n      <td>23</td>\n      <td>Cassandra Guerrero</td>\n      <td>aavail_basic</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>982</th>\n      <td>983</td>\n      <td>1</td>\n      <td>singapore</td>\n      <td>18</td>\n      <td>Dominic Trujillo</td>\n      <td>aavail_premium</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>983</th>\n      <td>984</td>\n      <td>1</td>\n      <td>singapore</td>\n      <td>-47</td>\n      <td>Westley Leon</td>\n      <td>aavail_basic</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>984</th>\n      <td>985</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>23</td>\n      <td>Juliette Fitzgerald</td>\n      <td>aavail_premium</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>985</th>\n      <td>986</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>26</td>\n      <td>Adam Burns</td>\n      <td>aavail_basic</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>986</th>\n      <td>987</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>23</td>\n      <td>Bianca Benitez</td>\n      <td>aavail_basic</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>987</th>\n      <td>988</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>20</td>\n      <td>Soren Leach</td>\n      <td>aavail_premium</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>988</th>\n      <td>989</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>33</td>\n      <td>Anakin Shepherd</td>\n      <td>aavail_basic</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>989</th>\n      <td>990</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>26</td>\n      <td>Liv Howard</td>\n      <td>aavail_basic</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>990</th>\n      <td>991</td>\n      <td>1</td>\n      <td>singapore</td>\n      <td>23</td>\n      <td>Mariah Rangel</td>\n      <td>aavail_basic</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>991</th>\n      <td>992</td>\n      <td>0</td>\n      <td>united_states</td>\n      <td>16</td>\n      <td>Mary Newton</td>\n      <td>aavail_premium</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>992</th>\n      <td>993</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>25</td>\n      <td>Asher Shelton</td>\n      <td>aavail_unlimited</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>993</th>\n      <td>994</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>38</td>\n      <td>Zahir Cabrera</td>\n      <td>aavail_unlimited</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>994</th>\n      <td>995</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>19</td>\n      <td>Steven Bean</td>\n      <td>aavail_basic</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>996</td>\n      <td>0</td>\n      <td>singapore</td>\n      <td>-46</td>\n      <td>Peyton Enriquez</td>\n      <td>aavail_unlimited</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>997</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>23</td>\n      <td>Amina Manning</td>\n      <td>aavail_basic</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>998</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>24</td>\n      <td>Brooks Ventura</td>\n      <td>aavail_unlimited</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>999</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>39</td>\n      <td>Nayeli Mathis</td>\n      <td>aavail_unlimited</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>1000</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>20</td>\n      <td>Cole Solis</td>\n      <td>aavail_unlimited</td>\n      <td>18</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows \u00d7 7 columns</p>\n</div>",
                        "text/plain": "     customer_id  is_subscriber        country  age        customer_name  \\\n0              1              1  united_states   21           Kasen Todd   \n1              2              0      singapore   30         Ensley Garza   \n2              3              0  united_states   21        Lillian Carey   \n3              4              1  united_states   20     Beau Christensen   \n4              5              1      singapore   21       Ernesto Gibson   \n5              6              1  united_states   21       Deshawn Murray   \n6              7              0      singapore   48          Daxton Tate   \n7              8              1  united_states   47         Tenley Small   \n8              9              0  united_states   21           Kyra Chase   \n9             10              0  united_states   26        London Barber   \n10            11              1  united_states   14          Rohan Rivas   \n11            12              0      singapore   39          Romina Ross   \n12            13              1  united_states   20        Liberty Moore   \n13            14              1  united_states   19          Yusuf Huber   \n14            15              0  united_states   22      Brantley Suarez   \n15            16              1  united_states   22           Skyler Ray   \n16            17              1  united_states   27            Joel Boyd   \n17            18              0      singapore   44         Zayne Torres   \n18            19              1  united_states   28         Madden Cross   \n19            20              1  united_states   20         Jabari Eaton   \n20            21              0      singapore   38      Claudia Hickman   \n21            22              0      singapore   41        Darwin Parker   \n22            23              1  united_states   22        Josiah Pruitt   \n23            24              0      singapore   23         Derek Landry   \n24            25              1      singapore   20         Sage Dejesus   \n25            26              1  united_states   24        Mikaela Adams   \n26            27              1  united_states   21            Jimmy Lee   \n27            28              1  united_states   18        Devin Hubbard   \n28            29              1      singapore   20        Kameron Miles   \n29            30              1  united_states   39   Francisco Jennings   \n..           ...            ...            ...  ...                  ...   \n970          971              1  united_states   26       Teagan Griffin   \n971          972              1  united_states   19           Diana Wise   \n972          973              1  united_states   44      Maryam Sheppard   \n973          974              1  united_states   13         Simon Stokes   \n974          975              0  united_states  -48           Leo Harper   \n975          976              1  united_states   22        Thatcher Best   \n976          977              1  united_states   24            Crew West   \n977          978              0      singapore   19         Boston Stark   \n978          979              1      singapore   24        Elisha Bishop   \n979          980              0  united_states   19      Forest Bradshaw   \n980          981              1  united_states   22      Carlos Mckenzie   \n981          982              1      singapore   23   Cassandra Guerrero   \n982          983              1      singapore   18     Dominic Trujillo   \n983          984              1      singapore  -47         Westley Leon   \n984          985              1  united_states   23  Juliette Fitzgerald   \n985          986              1  united_states   26           Adam Burns   \n986          987              1  united_states   23       Bianca Benitez   \n987          988              1  united_states   20          Soren Leach   \n988          989              1  united_states   33      Anakin Shepherd   \n989          990              1  united_states   26           Liv Howard   \n990          991              1      singapore   23        Mariah Rangel   \n991          992              0  united_states   16          Mary Newton   \n992          993              1  united_states   25        Asher Shelton   \n993          994              1  united_states   38        Zahir Cabrera   \n994          995              1  united_states   19          Steven Bean   \n995          996              0      singapore  -46      Peyton Enriquez   \n996          997              1  united_states   23        Amina Manning   \n997          998              1  united_states   24       Brooks Ventura   \n998          999              1  united_states   39        Nayeli Mathis   \n999         1000              1  united_states   20           Cole Solis   \n\n      subscriber_type  num_streams  \n0      aavail_premium           23  \n1    aavail_unlimited           12  \n2      aavail_premium           22  \n3        aavail_basic           19  \n4      aavail_premium           23  \n5      aavail_premium           20  \n6        aavail_basic           18  \n7      aavail_premium           20  \n8      aavail_premium           24  \n9        aavail_basic           20  \n10     aavail_premium           18  \n11   aavail_unlimited            1  \n12     aavail_premium           21  \n13       aavail_basic           22  \n14     aavail_premium           20  \n15       aavail_basic            5  \n16     aavail_premium           24  \n17       aavail_basic           17  \n18     aavail_premium           19  \n19     aavail_premium            5  \n20   aavail_unlimited            5  \n21       aavail_basic           19  \n22   aavail_unlimited           23  \n23       aavail_basic           14  \n24       aavail_basic           15  \n25     aavail_premium           23  \n26       aavail_basic           18  \n27       aavail_basic           14  \n28       aavail_basic           18  \n29   aavail_unlimited           21  \n..                ...          ...  \n970    aavail_premium           18  \n971  aavail_unlimited           22  \n972      aavail_basic           17  \n973  aavail_unlimited           17  \n974      aavail_basic           22  \n975      aavail_basic           18  \n976  aavail_unlimited           20  \n977    aavail_premium            8  \n978      aavail_basic           17  \n979    aavail_premium           19  \n980  aavail_unlimited           25  \n981      aavail_basic           18  \n982    aavail_premium           13  \n983      aavail_basic           20  \n984    aavail_premium           22  \n985      aavail_basic           19  \n986      aavail_basic           15  \n987    aavail_premium           17  \n988      aavail_basic           13  \n989      aavail_basic           21  \n990      aavail_basic           11  \n991    aavail_premium           20  \n992  aavail_unlimited           22  \n993  aavail_unlimited            7  \n994      aavail_basic           21  \n995  aavail_unlimited           14  \n996      aavail_basic           24  \n997  aavail_unlimited           17  \n998  aavail_unlimited           16  \n999  aavail_unlimited           18  \n\n[1000 rows x 7 columns]"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df.toPandas()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Here are some common operations that you might perform on a DataFrame"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--- printSchema()\nroot\n |-- customer_id: integer (nullable = true)\n |-- is_subscriber: integer (nullable = true)\n |-- country: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- customer_name: string (nullable = true)\n |-- subscriber_type: string (nullable = true)\n |-- num_streams: integer (nullable = true)\n\n--- show()\n+-----------+-------------+-------------+---+----------------+----------------+-----------+\n|customer_id|is_subscriber|      country|age|   customer_name| subscriber_type|num_streams|\n+-----------+-------------+-------------+---+----------------+----------------+-----------+\n|          1|            1|united_states| 21|      Kasen Todd|  aavail_premium|         23|\n|          2|            0|    singapore| 30|    Ensley Garza|aavail_unlimited|         12|\n|          3|            0|united_states| 21|   Lillian Carey|  aavail_premium|         22|\n|          4|            1|united_states| 20|Beau Christensen|    aavail_basic|         19|\n|          5|            1|    singapore| 21|  Ernesto Gibson|  aavail_premium|         23|\n|          6|            1|united_states| 21|  Deshawn Murray|  aavail_premium|         20|\n|          7|            0|    singapore| 48|     Daxton Tate|    aavail_basic|         18|\n|          8|            1|united_states| 47|    Tenley Small|  aavail_premium|         20|\n|          9|            0|united_states| 21|      Kyra Chase|  aavail_premium|         24|\n|         10|            0|united_states| 26|   London Barber|    aavail_basic|         20|\n|         11|            1|united_states| 14|     Rohan Rivas|  aavail_premium|         18|\n|         12|            0|    singapore| 39|     Romina Ross|aavail_unlimited|          1|\n|         13|            1|united_states| 20|   Liberty Moore|  aavail_premium|         21|\n|         14|            1|united_states| 19|     Yusuf Huber|    aavail_basic|         22|\n|         15|            0|united_states| 22| Brantley Suarez|  aavail_premium|         20|\n|         16|            1|united_states| 22|      Skyler Ray|    aavail_basic|          5|\n|         17|            1|united_states| 27|       Joel Boyd|  aavail_premium|         24|\n|         18|            0|    singapore| 44|    Zayne Torres|    aavail_basic|         17|\n|         19|            1|united_states| 28|    Madden Cross|  aavail_premium|         19|\n|         20|            1|united_states| 20|    Jabari Eaton|  aavail_premium|          5|\n+-----------+-------------+-------------+---+----------------+----------------+-----------+\nonly showing top 20 rows\n\n--- describe()\n+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n|summary|      customer_id|     is_subscriber|      country|               age| customer_name| subscriber_type|      num_streams|\n+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n|  count|             1000|              1000|         1000|              1000|          1000|            1000|             1000|\n|   mean|            500.5|             0.711|         null|            25.325|          null|            null|           17.695|\n| stddev|288.8194360957494|0.4535247343692345|         null|12.184655959067568|          null|            null|4.798020007877829|\n|    min|                1|                 0|    singapore|               -50|Aaliyah Duarte|    aavail_basic|                1|\n|    max|             1000|                 1|united_states|                50|   Zoie Cortes|aavail_unlimited|               29|\n+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n\n--- describe(Amount)\n+-------+-----------------+\n|summary|      num_streams|\n+-------+-----------------+\n|  count|             1000|\n|   mean|           17.695|\n| stddev|4.798020007877829|\n|    min|                1|\n|    max|               29|\n+-------+-----------------+\n\n"
                }
            ],
            "source": "# prints the schema\nprint(\"--- printSchema()\")\ndf.printSchema()\n\n# prints the table itself\nprint(\"--- show()\")\ndf.show()\n\n# show the statistics of all numerical columns\nprint(\"--- describe()\")\ndf.describe().show()\n\n# show the statistics of one specific column\nprint(\"--- describe(Amount)\")\ndf.describe(\"num_streams\").show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": "## Transformations on DataFrames\n\n- They are still **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n- They transform a DataFrame into another because DataFrames are also **immutable**.\n- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n\n\nLets read in in the AAVAIL dataset that we have been working with to demonstrate the transformations."
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "slideshow": {
                    "slide_type": "-"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n|summary|      customer_id|     is_subscriber|      country|               age| customer_name| subscriber_type|      num_streams|\n+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n|  count|             1000|              1000|         1000|              1000|          1000|            1000|             1000|\n|   mean|            500.5|             0.711|         null|            25.325|          null|            null|           17.695|\n| stddev|288.8194360957494|0.4535247343692345|         null|12.184655959067568|          null|            null|4.798020007877829|\n|    min|                1|                 0|    singapore|               -50|Aaliyah Duarte|    aavail_basic|                1|\n|    max|             1000|                 1|united_states|                50|   Zoie Cortes|aavail_unlimited|               29|\n+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n\n"
                }
            ],
            "source": "# read CSV\ndf_aavail = spark.read.csv(os.path.join(data_dir,'aavail-target.csv'),\n                           header=True,       \n                           quote='\"',         \n                           sep=\",\",          \n                           inferSchema=True)\ndf_aavail.describe().show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "notes"
                }
            },
            "source": "## Remove one or more columns"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+-------+------------------+-------------+------------------+----------------+-----------------+\n|summary|     is_subscriber|      country|               age| subscriber_type|      num_streams|\n+-------+------------------+-------------+------------------+----------------+-----------------+\n|  count|              1000|         1000|              1000|            1000|             1000|\n|   mean|             0.711|         null|            25.325|            null|           17.695|\n| stddev|0.4535247343692345|         null|12.184655959067568|            null|4.798020007877829|\n|    min|                 0|    singapore|               -50|    aavail_basic|                1|\n|    max|                 1|united_states|                50|aavail_unlimited|               29|\n+-------+------------------+-------------+------------------+----------------+-----------------+\n\n+----------------+-----+\n| subscriber_type|count|\n+----------------+-----+\n|  aavail_premium|  331|\n|aavail_unlimited|  302|\n|    aavail_basic|  367|\n+----------------+-----+\n\n"
                }
            ],
            "source": "columns_to_drop = ['customer_id','customer_name']\ndf_aavail = df_aavail.drop(*columns_to_drop)\ndf_aavail.describe().show()\ndf_aavail.groupBy(\"subscriber_type\").count().show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Transformations on a feature matrix\n\nThe following example demonstrates how to deal with categorical features and scale continuous ones"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "slideshow": {
                    "slide_type": "notes"
                }
            },
            "outputs": [],
            "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer\nfrom pyspark.ml.feature import StandardScaler, VectorAssembler\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml import Pipeline\n\n## scale the continuous features\nva = VectorAssembler(inputCols=[\"age\", \"num_streams\"], outputCol=\"cont_features\")\nss = standardScaler = StandardScaler(inputCol=\"cont_features\", outputCol=\"cont_scaled\")\n\n## categorical variable transformation\ncat_cols = [\"country\",\"subscriber_type\"]\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in cat_cols]\nencoders = [OneHotEncoder(inputCol=column+\"_index\", outputCol=column+\"_oh\") for column in cat_cols]\n\n## assemple the features for input into the ML model\nassembler = VectorAssembler(inputCols=[\"cont_scaled\", \"country_oh\",\"subscriber_type_oh\"], outputCol=\"features\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "MLlib Estimators and Transformers use the same API for specifying parameters. There are two basic methods to pass parameters:\n\n* **Param** - A named parameter with a self-contained documentation\n* **ParamMap** - Is a set of (parameter, value) pairs"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "## setup a model\ngbt = GBTClassifier(labelCol=\"is_subscriber\", featuresCol=\"features\")\nparamMap = {gbt.maxIter: 20}"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Setup the pipeline and train the model"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+--------------------+-------------+--------------------+--------------------+----------+\n|            features|is_subscriber|       rawPrediction|         probability|prediction|\n+--------------------+-------------+--------------------+--------------------+----------+\n|[1.72347910934425...|            1|[-0.7104551723677...|[0.19451890992497...|       1.0|\n|(5,[0,1],[2.46211...|            0|[0.77502044150328...|[0.82491963653378...|       0.0|\n|[1.72347910934425...|            0|[-0.7104551723677...|[0.19451890992497...|       1.0|\n|[1.64140867556596...|            1|[-0.9291971347680...|[0.13489032271470...|       1.0|\n|[1.72347910934425...|            1|[0.00233192525665...|[0.50116596051488...|       0.0|\n|[1.72347910934425...|            1|[-0.7703335442768...|[0.17643832065912...|       1.0|\n|[3.93938082135830...|            0|[0.74986801956411...|[0.81753510406590...|       0.0|\n|[3.85731038758001...|            1|[-0.9215852644165...|[0.13667675103731...|       1.0|\n|[1.72347910934425...|            0|[-0.4368188087314...|[0.29449795362552...|       1.0|\n|[2.13383127823575...|            0|[-0.8756884670117...|[0.14787361056083...|       1.0|\n|[1.14898607289617...|            1|[-1.2076283318498...|[0.08201667364617...|       1.0|\n|(5,[0,1],[3.20074...|            0|[1.36797713189510...|[0.93911518070094...|       0.0|\n|[1.64140867556596...|            1|[-0.8905262171262...|[0.14417322842494...|       1.0|\n|[1.55933824178766...|            1|[-1.0286924396352...|[0.11330830445383...|       1.0|\n|[1.80554954312255...|            0|[-0.6576928685452...|[0.21158701096329...|       1.0|\n|[1.80554954312255...|            1|[-1.4203671689725...|[0.05516225204124...|       1.0|\n|[2.21590171201404...|            1|[-0.6766933461545...|[0.20531724057277...|       1.0|\n|[3.61109908624511...|            0|[0.46428590396626...|[0.71678544647694...|       0.0|\n|[2.29797214579234...|            1|[-1.0136431193885...|[0.11636767755533...|       1.0|\n|[1.64140867556596...|            1|[-1.4405485892111...|[0.05309594675816...|       1.0|\n+--------------------+-------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "## run the whole pipeline\npipe = Pipeline(stages=indexers+encoders+[va,ss,assembler,gbt])\n_result = pipe.fit(df_aavail,paramMap).transform(df_aavail)\nresult = _result.select(\"features\", \"is_subscriber\", \"rawPrediction\", \"probability\",\"prediction\")\nresult.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Now the same procedure with a train-test split, cross-validations and grid-search"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "A train-test split can be carried out with TrainValidationSplit. Cross Validation is accomplished in Spark MLlib using the CrossValidator() object. A data set is split into a set of folds which are used as separate training and test datasets. The CrossValidator computes the average evaluation metric for the k models produced by fitting the Estimator on the k different (training, test) dataset pairs. This helps identify the best ParamMap, which is then used to re-fit the Estimator with the entire dataset."
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model trained\n"
                }
            ],
            "source": "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\ntrain, test = df_aavail.randomSplit([0.8, 0.2], seed=42)\n\ngbt = GBTClassifier(labelCol=\"is_subscriber\", featuresCol=\"features\")\nparamGrid = ParamGridBuilder() \\\n    .addGrid(gbt.maxIter, [10, 20]) \\\n    .addGrid(gbt.stepSize, [0.01, 0.1]) \\\n    .build()\n\npipe = Pipeline(stages=indexers+encoders+[va,ss,assembler])\npipeline_model = pipe.fit(train)\nprepped_train = pipeline_model.transform(train)\nprepped_test = pipeline_model.transform(test)\n\ncrossval = CrossValidator(estimator=gbt,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(labelCol=\"is_subscriber\"),\n                          numFolds=3)\n\n# Run cross-validation, and choose the best set of parameters.\ncvModel = crossval.fit(prepped_train)\nprint(\"model trained\")"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+--------------------+-------------+--------------------+--------------------+----------+\n|            features|is_subscriber|       rawPrediction|         probability|prediction|\n+--------------------+-------------+--------------------+--------------------+----------+\n|[-4.0596848816199...|            0|[0.51824301405819...|[0.73817141398644...|       0.0|\n|[-3.9733086075428...|            0|[-0.6025984312611...|[0.23055201394382...|       1.0|\n|(5,[0,1],[-3.9733...|            0|[0.56437071574429...|[0.75560655995383...|       0.0|\n|(5,[0,1],[1.46839...|            0|[0.53723327942381...|[0.74544540342558...|       0.0|\n|(5,[0,1],[1.46839...|            0|[0.53723327942381...|[0.74544540342558...|       0.0|\n|(5,[0,1],[1.64114...|            0|[0.47340782396592...|[0.72047432841242...|       0.0|\n|(5,[0,1],[1.72752...|            0|[0.47340782396592...|[0.72047432841242...|       0.0|\n|(5,[0,1],[1.72752...|            0|[0.46519184595527...|[0.71715312177940...|       0.0|\n|[1.81390175561740...|            0|[0.06472327735034...|[0.53231652552520...|       0.0|\n|[1.81390175561740...|            0|[0.99064920327626...|[0.87881950495640...|       0.0|\n|(5,[0,1],[1.81390...|            0|[0.47340782396592...|[0.72047432841242...|       0.0|\n|(5,[0,1],[1.90027...|            0|[0.47340782396592...|[0.72047432841242...|       0.0|\n|(5,[0,1],[1.90027...|            0|[0.47340782396592...|[0.72047432841242...|       0.0|\n|(5,[0,1],[1.90027...|            0|[0.57608563010346...|[0.75990728299889...|       0.0|\n|(5,[0,1],[1.98665...|            0|[0.49364904209583...|[0.72855391317279...|       0.0|\n|[2.24578312600249...|            0|[0.08496449548025...|[0.54238031610623...|       0.0|\n|[2.50491194823355...|            0|[0.49364904209583...|[0.72855391317279...|       0.0|\n|(5,[0,1],[2.59128...|            0|[0.49364904209583...|[0.72855391317279...|       0.0|\n|[2.76404077046461...|            0|[0.08496449548025...|[0.54238031610623...|       0.0|\n|[2.85041704454163...|            0|[0.61283156443516...|[0.77305861912168...|       0.0|\n+--------------------+-------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "prediction = cvModel.transform(prepped_test)\nresult = prediction.select(\"features\", \"is_subscriber\", \"rawPrediction\", \"probability\",\"prediction\")\nresult.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Spark Supervised Learning"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Spark MLlib has a number of available supervised learning algorithms\u2014specifically those used for classification and regression. Many of the commonly used algorithms have been implemented including: random forests, gradient boosted trees, linear support vector machines and even basic multilayer perceptrons."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Spark Mlib has fewer models and algorithms to choose from compared to scikit-learn\u2019s supervised learning, but many of the most popular methods are present. Both random forests and gradient boosted trees are models used in production and should be on your radar when comparing models. They both use decision trees as a base model."
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "import pyspark as ps\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "import requests\ntext = requests.get('https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt').text\n\nwith open(\"sample_libsvm_data.txt\", \"w\") as text_file:\n    text_file.write(text)"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(692,[127,128,129...|\n|  1.0|(692,[158,159,160...|\n|  1.0|(692,[124,125,126...|\n|  1.0|(692,[152,153,154...|\n|  1.0|(692,[151,152,153...|\n+-----+--------------------+\nonly showing top 5 rows\n\n+--------------+-----+--------------------+\n|predictedLabel|label|            features|\n+--------------+-----+--------------------+\n|           0.0|  0.0|(692,[100,101,102...|\n|           0.0|  0.0|(692,[124,125,126...|\n|           0.0|  0.0|(692,[126,127,128...|\n|           0.0|  0.0|(692,[126,127,128...|\n|           0.0|  0.0|(692,[152,153,154...|\n+--------------+-----+--------------------+\nonly showing top 5 rows\n\nTest Error = 0.0333333\nRandomForestClassificationModel (uid=RandomForestClassifier_04ee130a95c9) with 10 trees\n"
                }
            ],
            "source": "# Index and parse the data file, converting it to a DataFrame\ndata = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")\ndata.show(5)\n\n# Index labels, adding metadata to the labels columns\n# Fit on whole dataset to include all labels in index\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n\n# Automatically identify categorical features and index them.\n# Set maxCategories so features with > 4 distinct values are treates as continuous.\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n\n# Train model. This also runs the Indexers\nmodel = pipeline.fit(trainingData)\n\n# Make predictions\npredictions = model.transform(testData)\n\n# Select example rows to display\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", \n                                              predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Spark Unsupervised Learning"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Spark MLlib has several available tools for unsupervised learning\u2014namely dimension reduction and clustering. For clustering, K-means and Gaussian Mixture Models (GMMs) are the main tools. Latent Dirichlet Allocation (LDA) is available as a tool for clustering over documents of natural language. This is a particularly important tool since the size of NLP datasets can often make single-node computation challenging."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "For dimension reduction, two of the most frequently used tools are PCA and the Chi-Squared Feature Selector. All of the tools in the unsupervised learning category take the form of a transformer or an estimator and, in keeping with the scikit-learn API, they too can be assembled in pipelines."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        },
        "rise": {
            "autolaunch": true,
            "enable_chalkboard": true,
            "theme": "sky"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}