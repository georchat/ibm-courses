{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# START CODE BLOCK\n# cos2file - takes an object from Cloud Object Storage and writes it to file on container file system.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: project object defined in project token\n# data_path: the directory to write the file\n# filename: name of the file in COS\n\nimport os\ndef cos2file(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n    open( data_dir + '/' + filename, 'wb').write(p.get_file(filename).read())\n\n# file2cos - takes file on container file system and writes it to an object in Cloud Object Storage.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: prooject object defined in project token\n# data_path: the directory to read the file from\n# filename: name of the file on container file system\n\nimport os\ndef file2cos(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    path_to_file = data_dir + '/' + filename\n    if os.path.exists(path_to_file):\n        file_object = open(path_to_file, 'rb')\n        p.save_data(filename, file_object, set_project_asset=True, overwrite=True)\n    else:\n        print(\"file2cos error: File not found\")\n# END CODE BLOCK"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "cos2file(project, '/data', 'aavail-target.csv')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Docker Tutorial\n\nBefore you get started.  If you feel like you still need some practice getting a feel for docker try the tutrial for beginners in the Docker tutorial, before starting this tutorial.  Docker is intuitive so going through a few examples will be all that it takes to get comfortable.\n\n* [Docker Tutorials](https://github.com/docker/labs/blob/master/beginner/readme.md)\n\nThis tutorial is loosly based on the second tutorial, `Webapps with Docker`, so going through both of those tutorials along with this one will provide a lot of context for how to use Docker in a number of different ways.\n\n> You will need to run through this tutorial with access to a termminal.  Jupyter lab or an open terminal will work.  We will create some of the files you need from within this notebook, but Docker is a command line tool."
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: joblib in /opt/conda/envs/Python36/lib/python3.6/site-packages (0.14.1)\r\n"
                }
            ],
            "source": "!pip install joblib"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "import os\nimport sys\nimport joblib\nimport requests\nimport numpy as np\nimport pandas as pd\n\nfrom collections import Counter\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn import ensemble"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "## preprocessing pipeline\nnumeric_features = ['age', 'num_streams']\nnumeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n                                      ('scaler', StandardScaler())])\n\ncategorical_features = ['country', 'subscriber_type']\ncategorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n                                               ('cat', categorical_transformer, categorical_features)])\n\n\ndef load_aavail_data():\n    data_dir = os.path.join(\"..\",\"..\",\"data\")\n    df = pd.read_csv(os.path.join(data_dir,r\"aavail-target.csv\"))\n\n    ## pull out the target and remove uneeded columns\n    _y = df.pop('is_subscriber')\n    y = np.zeros(_y.size)\n    y[_y==0] = 1 \n    df.drop(columns=['customer_id','customer_name'],inplace=True)\n    return(df,y)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Docker command reference\n\nHere is a quick reference to keep your Docker commands accessable.\n\n| command | description |\n|:--|:--|\n|`docker container ls`| # List all running containers|\n|`docker ps` | # List all running containers|\n|`docker container ls -a` |  # List all containers, even those not running|\n|`docker container stop CONTAINER_ID_OR_NAME` | # Gracefully stop the specified container|\n|`docker container kill CONTAINER_ID_OR_NAME` | # Force shutdown of the specified container|\n|`docker container rm CONTAINER_ID_OR_NAME`  |   # Remove specified container from this machine|\n|`docker container rm $(docker container ls -a -q)` | # Remove all containers|\n|`docker image ls -a`  | # List all images on this machine|\n|`docker image rm IMAGE_ID_OR_NAME` | # Remove specified image from this machine|\n|`docker image rm $(docker image ls -a -q)`   |# Remove all images from this machine|\n|`docker login` |# Log in this CLI session using your Docker credentials|"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/home/dsxuser/work/docker-tutorial\n"
                }
            ],
            "source": "## make a directory for the tutorial\nif not os.path.isdir(\"docker-tutorial\"):\n    os.mkdir(\"docker-tutorial\")\n    \nif os.path.split(os.getcwd())[-1] != 'docker-tutorial':  \n    os.chdir(\"docker-tutorial\")\nprint(os.getcwd())"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Persist a machine learning model\n\nVist the docs to learn more about [model persistence in scikit-learn](https://scikit-learn.org/stable/modules/model_persistence.html).  Be careful with sensitive data and pickle files since the data can easily be extracted."
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "              precision    recall  f1-score   support\n\n         0.0       0.83      0.89      0.86       142\n         1.0       0.68      0.55      0.61        58\n\n   micro avg       0.80      0.80      0.80       200\n   macro avg       0.76      0.72      0.74       200\nweighted avg       0.79      0.80      0.79       200\n\n"
                },
                {
                    "data": {
                        "text/plain": "['aavail-rf.joblib']"
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "## load data (you may need to adjust the location of the data to match your system)\nX,y = load_aavail_data()\n\n## train test split check model performance (assumes you have already grid-searched to tune model)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nparams = {'n_estimators': 100,'max_depth':2}   \nclf = ensemble.RandomForestClassifier(**params)\npipe = Pipeline(steps=[('pre', preprocessor),\n                       ('clf',clf)])\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint(classification_report(y_test,y_pred))\n\n## retrain using all of the data\npipe.fit(X, y)\nsaved_model = 'aavail-rf.joblib'\njoblib.dump(pipe, saved_model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Create a simple flask app"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already up-to-date: flask in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.1.2)\nRequirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from flask) (1.0.1)\nRequirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from flask) (2.11.2)\nRequirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from flask) (1.1.0)\nRequirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from flask) (7.0)\nRequirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from Jinja2>=2.10.1->flask) (1.1.0)\n"
                }
            ],
            "source": "!pip install -U flask"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Overwriting app.py\n"
                }
            ],
            "source": "%%writefile app.py\n\nfrom flask import Flask, jsonify, request\nimport joblib\nimport socket\nimport json\nimport pandas as pd\nimport os\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    html = \"<h3>Hello {name}!</h3>\" \\\n           \"<b>Hostname:</b> {hostname}<br/>\"\n    return html.format(name=os.getenv(\"NAME\", \"world\"), hostname=socket.gethostname())\n\n@app.route('/predict', methods=['GET','POST'])\ndef predict():\n    \n    ## input checking\n    if not request.json:\n        print(\"ERROR: API (predict): did not receive request data\")\n        return jsonify([])\n\n    query = request.json\n    query = pd.DataFrame(query)\n    \n    if len(query.shape) == 1:\n         query = query.reshape(1, -1)\n\n    y_pred = model.predict(query)\n    \n    return(jsonify(y_pred.tolist()))        \n            \nif __name__ == '__main__':\n    saved_model = 'aavail-rf.joblib'\n    model = joblib.load(saved_model)\n    app.run(host='0.0.0.0', port=8080,debug=True)\n    # to run it with python (not docker) use localhost\n    #app.run(host='localhost', port=8080,debug=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Test the flask app\n\nMove into your `docker-tutorial` directory\n\n```bash\n$ cd docker-tutorial\n```\n\nStart the app\n\n```bash\n$ python app.py\n```\n\nThen go to [http://localhost:8080/](http://localhost:8080/)\n\nStop the server.  We will relaunch it in a few moments from within Docker."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Create the DockerFile\n\nBefore we build the DockerFile first we need to create a requirement.txt"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Overwriting requirements.txt\n"
                }
            ],
            "source": "%%writefile requirements.txt\n\ncython\nnumpy\nflask\npandas\nscikit-learn"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Overwriting Dockerfile\n"
                }
            ],
            "source": "%%writefile Dockerfile\n\n# Use an official Python runtime as a parent image\nFROM python:3.7.5-stretch\n\nRUN apt-get update && apt-get install -y \\\npython3-dev \\\nbuild-essential    \n        \n# Set the working directory to /app\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nADD . /app\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --upgrade pip\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Make port 80 available to the world outside this container\nEXPOSE 80\n\n# Define environment variable\nENV NAME World\n\n# Run app.py when the container launches\nCMD [\"python\", \"app.py\"]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Build the Docker image and run it\n\nStep one: build the image (from the directory that was created with this notebook)\n \n```bash\n    ~$ cd docker-tutorial\n    ~$ docker build -t example-ml-app .\n```\n\nCheck that the image is there.\n\n```bash\n    ~$ docker image ls\n```\n\nYou may notice images that you no longer use.  You may delete them with\n\n```bash\n    ~$ docker image rm IMAGE_ID_OR_NAME\n```\n\nRun the container\n\n```bash\ndocker run -p 4000:8080 example-ml-app\n```"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Test the running app\n\nFirst go to [http://localhost:4000/](http://localhost:4000/) to ensure the app is running and accessible."
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>age</th>\n      <th>subscriber_type</th>\n      <th>num_streams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>united_states</td>\n      <td>28</td>\n      <td>aavail_premium</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>united_states</td>\n      <td>30</td>\n      <td>aavail_basic</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>singapore</td>\n      <td>33</td>\n      <td>aavail_basic</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>united_states</td>\n      <td>24</td>\n      <td>aavail_basic</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>singapore</td>\n      <td>39</td>\n      <td>aavail_unlimited</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "         country  age   subscriber_type  num_streams\n0  united_states   28    aavail_premium            9\n1  united_states   30      aavail_basic           19\n2      singapore   33      aavail_basic           14\n3  united_states   24      aavail_basic           33\n4      singapore   39  aavail_unlimited           20"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "## create some new data\nX_new_data = {}\nX_new_data['country'] = ['united_states','united_states','singapore','united_states','singapore']\nX_new_data['age'] = [28,30,33,24,39]\nX_new_data['subscriber_type'] = ['aavail_premium','aavail_basic','aavail_basic','aavail_basic','aavail_unlimited']\nX_new_data['num_streams'] = [9,19,14,33,20]\nX_new = pd.DataFrame(X_new_data)\nX_new.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import requests\nfrom ast import literal_eval\n\n\n## data needs to be in dict format for JSON\nquery = X_new.to_dict()\n\n## test the Flask API\n#port = 8080\n#r = requests.post('http://localhost:{}/predict'.format(port),json=query)\n\n## test the Docker API\nport = 4000\nr = requests.post('http://0.0.0.0:{}/predict'.format(port),json=query)\n\nresponse = literal_eval(r.text)\nprint(response)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Continued learning\n\nIn this tutorial we showed how to add a `predict` endpoint to the flask app.  Go back and edit the flask app to add a training endpoint that accepts new data as input."
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}