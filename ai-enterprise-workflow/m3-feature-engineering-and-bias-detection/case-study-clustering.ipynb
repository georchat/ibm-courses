{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# CASE STUDY - unsupervised learning\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: joblib in /opt/conda/envs/Python36/lib/python3.6/site-packages (0.14.1)\r\n"
                }
            ],
            "source": "!pip install joblib"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already up-to-date: imbalanced-learn in /opt/conda/envs/Python36/lib/python3.6/site-packages (0.6.2)\r\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn) (0.14.1)\r\nRequirement already satisfied, skipping upgrade: numpy>=1.11 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn) (1.15.4)\r\nRequirement already satisfied, skipping upgrade: scikit-learn>=0.22 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn) (0.22.2.post1)\r\nRequirement already satisfied, skipping upgrade: scipy>=0.17 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn) (1.2.0)\r\n"
                }
            ],
            "source": "!pip install -U imbalanced-learn"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n"
                }
            ],
            "source": "import os\nimport joblib\nimport time\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.cluster import KMeans, SpectralClustering\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.mixture import BayesianGaussianMixture\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nimport imblearn.pipeline as pl\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE, SVMSMOTE\n    \nplt.style.use('seaborn')\n%matplotlib inline"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Make this notebook run in IBM Watson"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# START CODE BLOCK\n# cos2file - takes an object from Cloud Object Storage and writes it to file on container file system.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: project object defined in project token\n# data_path: the directory to write the file\n# filename: name of the file in COS\n\nimport os\ndef cos2file(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n    open( data_dir + '/' + filename, 'wb').write(p.get_file(filename).read())\n\n# file2cos - takes file on container file system and writes it to an object in Cloud Object Storage.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: prooject object defined in project token\n# data_path: the directory to read the file from\n# filename: name of the file on container file system\n\nimport os\ndef file2cos(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    path_to_file = data_dir + '/' + filename\n    if os.path.exists(path_to_file):\n        file_object = open(path_to_file, 'rb')\n        p.save_data(filename, file_object, set_project_asset=True, overwrite=True)\n    else:\n        print(\"file2cos error: File not found\")\n# END CODE BLOCK"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "cos2file(project, '/data', 'aavail-target.csv')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Synopsis\n\n  > We are now going to predict customer retention.  There are many models and many transforms to consider.  Use your\n    knowledge of pipelines and functions to ensure that your code makes it easy to compare and iterate.  \n    \n  > Marketing has asked you to make a report on customer retention.  They would like you to come up with information     that can be used to improve current marketing strategy efforts.  The current plan is for marketing at AAVAIL to\n    collect more features on subscribers the and they would like to use your report as a proof-of-concept in order to     get buyin for this effort.\n  \n## Outline\n\n1. Create a churn prediction baseline model\n2. Use clustering as part of your prediction pipeline\n3. \n4. Run and experiment to see if re-sampling techniques improve your model\n\n## Data\n\nHere we load the data as we have already done.\n\n`aavail-target.csv`"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>is_subscriber</th>\n      <th>country</th>\n      <th>age</th>\n      <th>customer_name</th>\n      <th>subscriber_type</th>\n      <th>num_streams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>21</td>\n      <td>Kasen Todd</td>\n      <td>aavail_premium</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>singapore</td>\n      <td>30</td>\n      <td>Ensley Garza</td>\n      <td>aavail_unlimited</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>united_states</td>\n      <td>21</td>\n      <td>Lillian Carey</td>\n      <td>aavail_premium</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>20</td>\n      <td>Beau Christensen</td>\n      <td>aavail_basic</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>singapore</td>\n      <td>21</td>\n      <td>Ernesto Gibson</td>\n      <td>aavail_premium</td>\n      <td>23</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   customer_id  is_subscriber        country  age     customer_name  \\\n0            1              1  united_states   21        Kasen Todd   \n1            2              0      singapore   30      Ensley Garza   \n2            3              0  united_states   21     Lillian Carey   \n3            4              1  united_states   20  Beau Christensen   \n4            5              1      singapore   21    Ernesto Gibson   \n\n    subscriber_type  num_streams  \n0    aavail_premium           23  \n1  aavail_unlimited           12  \n2    aavail_premium           22  \n3      aavail_basic           19  \n4    aavail_premium           23  "
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "data_dir = os.path.join(\"..\",\"data\")\ndf = pd.read_csv(os.path.join(data_dir, r\"aavail-target.csv\"))\ndf.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>age</th>\n      <th>subscriber_type</th>\n      <th>num_streams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>united_states</td>\n      <td>21</td>\n      <td>aavail_premium</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>singapore</td>\n      <td>30</td>\n      <td>aavail_unlimited</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>united_states</td>\n      <td>21</td>\n      <td>aavail_premium</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>united_states</td>\n      <td>20</td>\n      <td>aavail_basic</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>singapore</td>\n      <td>21</td>\n      <td>aavail_premium</td>\n      <td>23</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "         country  age   subscriber_type  num_streams\n0  united_states   21    aavail_premium           23\n1      singapore   30  aavail_unlimited           12\n2  united_states   21    aavail_premium           22\n3  united_states   20      aavail_basic           19\n4      singapore   21    aavail_premium           23"
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "## pull out the target and remove uneeded columns\n_y = df.pop('is_subscriber')\ny = np.zeros(_y.size)\ny[_y==0] = 1 \ndf.drop(columns=['customer_id','customer_name'], inplace=True)\ndf.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### QUESTION 1\n\nCreate a stratified train test split of the data"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(750, 4) (750,)\n(250, 4) (250,)\n"
                }
            ],
            "source": "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.25, stratify=y, random_state=1)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### QUESTION 2\n\nCreate a baseline model.  We are going to test whether clustering followed by a model improves the results.  The we will test whether re-sampling techniques provide improvements.  Use a pipeline or another method, but create a baseline model given the data. Here is the ColumnTransformer we have used before."
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "## preprocessing pipeline\nnumeric_features = ['age', 'num_streams']\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())])\n\ncategorical_features = ['country', 'subscriber_type']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "prep-->log\nf1_score 0.562\nprep-->svm\nf1_score 0.609\nprep-->rf\nf1_score 0.533\n"
                },
                {
                    "data": {
                        "text/plain": "{'log__C': 0.1,\n 'log__penalty': 'l2',\n 'svm__C': 5.0,\n 'svm__gamma': 0.1,\n 'rf__criterion': 'gini',\n 'rf__max_depth': 4,\n 'rf__n_estimators': 20}"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "best_params = {}\n\n# Logistic Regression\npipe_log = Pipeline([(\"prep\", preprocessor), (\"log\", LogisticRegression())])\n\nparam_grid_log = [{\n    'log__C': [0.01,0.1,0.5,1.0,1.5,5.0,10.0],\n    'log__penalty': [\"l1\", \"l2\"]\n}]\n\ngrid_search_log = GridSearchCV(pipe_log, param_grid=param_grid_log, cv=5, n_jobs=-1)\ngrid_search_log.fit(X_train, y_train)\n\ny_pred = grid_search_log.predict(X_test)\nprint(\"-->\".join(pipe_log.named_steps.keys()))\nbest_params = grid_search_log.best_params_\nprint(\"f1_score\", round(f1_score(y_test, y_pred,average='binary'),3))\n\n\n# SVM\npipe_svm = Pipeline([(\"prep\", preprocessor), (\"svm\", SVC(kernel='rbf', class_weight='balanced'))])\n\nparam_grid_svm = [{\n    'svm__C': [0.01,0.1,0.5,1.0,1.5,5.0,10.0],\n    'svm__gamma': [0.001,0.01,0.1]\n}]\n\ngrid_search_svm = GridSearchCV(pipe_svm, param_grid=param_grid_svm, cv=5, n_jobs=-1)\ngrid_search_svm.fit(X_train, y_train)\n\ny_pred = grid_search_svm.predict(X_test)\nprint(\"-->\".join(pipe_svm.named_steps.keys()))\nbest_params = dict(best_params, **grid_search_svm.best_params_)\nprint(\"f1_score\", round(f1_score(y_test, y_pred, average='binary'),3))\n\n\n# Random Forest\npipe_rf = Pipeline([(\"prep\", preprocessor), (\"rf\", RandomForestClassifier())])\n\nparam_grid_rf = {\n    'rf__n_estimators': [20,50,100,150],\n    'rf__max_depth': [4, 5, 6, 7, 8],\n    'rf__criterion': ['gini', 'entropy']\n}\n\ngrid_search_rf = GridSearchCV(pipe_rf, param_grid=param_grid_rf, cv=5, n_jobs=-1)\ngrid_search_rf.fit(X_train, y_train)\n\ny_pred = grid_search_rf.predict(X_test)\nprint(\"-->\".join(pipe_rf.named_steps.keys()))\nbest_params = dict(best_params, **grid_search_rf.best_params_)\nprint(\"f1_score\",round(f1_score(y_test, y_pred,average='binary'),3))\n\n###\nbest_params"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### QUESTION 3\n\nThe next part is to create version of the classifier that uses identified clusters.  Here is a class to get you started.  It is a transformer like those that we have been working with.  There is an example of how to use it just below.  In this example 4 clusters were specified and their one-hot encoded versions were appended to the feature matrix.  Now using pipelines and/or functions compare the performance using cluster profiling as part of your matrix to the baseline.  You may compare multiple models and multiple clustering algorithms here."
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(750, 7)\n(750, 11)\n(750, 7)\n(750, 11)\n"
                }
            ],
            "source": "class KmeansTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, k=4):\n        self.km = KMeans(n_clusters=k, n_init=20)\n        \n    def transform(self, X, *_):\n        labels = self.km.predict(X)\n        enc = OneHotEncoder(categories='auto')\n        oh_labels = enc.fit_transform(labels.reshape(-1,1))\n        oh_labels = oh_labels.todense()\n        return(np.hstack((X,oh_labels)))\n\n    def fit(self,X,y=None,*_):\n        self.km.fit(X)\n        labels = self.km.predict(X)\n        self.silhouette_score = round(silhouette_score(X,labels,metric='mahalanobis'),3)\n        return(self)\n\nclass GmmTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, k=4):\n        self.gmm = BayesianGaussianMixture(n_components=k,covariance_type='full', max_iter=500, n_init=10, warm_start=True)        \n    def transform(self, X,*_):\n        probs = self.gmm.predict_proba(X) + np.finfo(float).eps\n        return(np.hstack((X,probs)))\n        \n    def fit(self,X,y=None,*_):\n        self.gmm.fit(X)\n        labels = self.gmm.predict(X)\n        self.silhouette_score = round(silhouette_score(X,labels,metric='mahalanobis'),3)\n        return(self)\n    \n## example for GMM  \npreprocessor.fit(X_train)\nX_train_pre = preprocessor.transform(X_train)    \ngt = GmmTransformer(4)\ngt.fit(X_train_pre)\nX_train_gmm = gt.transform(X_train_pre)\nprint(X_train_pre.shape)  \nprint(X_train_gmm.shape)\n\n## example for kmeans\npreprocessor.fit(X_train)\nX_train_pre = preprocessor.transform(X_train)    \nkt = KmeansTransformer(4)\nkt.fit(X_train_pre)\nX_train_kmeans = kt.transform(X_train_pre)\nprint(X_train_pre.shape)\nprint(X_train_kmeans.shape)"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "def run_clustering_pipeline(X_train, y_train, X_test, y_test, smodel, umodel, best_params, preprocessor):\n    fscores,sscores = [],[]\n    for n_clusters in np.arange(3, 8):\n        \n        if smodel==\"rf\":\n            clf = RandomForestClassifier(n_estimators=best_params['rf__n_estimators'], criterion=best_params['rf__criterion'], max_depth=best_params['rf__max_depth'])\n        elif smodel==\"log\":\n            clf = LogisticRegression(C=best_params['log__C'], penalty=best_params[\"log__penalty\"])\n        elif smodel==\"svm\":\n            clf = SVC(C=best_params['svm__C'], gamma=best_params['svm__gamma'])\n        else:\n            raise Exception(\"invalid supervised learning model\")\n        \n        if umodel==\"kmeans\":\n            cluster = KmeansTransformer(k=n_clusters)\n        elif umodel==\"gmm\":\n            cluster = GmmTransformer(k=n_clusters)\n        else:\n            raise Exception(\"invalid unsupervised learning model\")\n            \n        pipe = Pipeline(steps=[('pre', preprocessor), ('cluster', cluster), ('clf', clf)])  \n        \n        pipe.fit(X_train, y_train)\n        y_pred = pipe.predict(X_test)\n        \n        fscore = round(f1_score(y_test, y_pred, average='binary'),3)\n        sscore = pipe['cluster'].silhouette_score\n        \n        fscores.append(fscore)\n        sscores.append(sscore)\n        \n    return fscores, sscores"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'svm-kmeans-f': [0.538, 0.538, 0.538, 0.538, 0.538],\n 'svm-kmeans-s': [0.23, 0.186, 0.231, 0.297, 0.334],\n 'svm-gmm-f': [0.538, 0.538, 0.538, 0.538, 0.538],\n 'svm-gmm-s': [0.265, 0.279, 0.292, 0.388, 0.419],\n 'rf-kmeans-f': [0.534, 0.538, 0.533, 0.548, 0.512],\n 'rf-kmeans-s': [0.23, 0.186, 0.231, 0.297, 0.335],\n 'rf-gmm-f': [0.542, 0.538, 0.534, 0.542, 0.538],\n 'rf-gmm-s': [0.265, 0.279, 0.293, 0.369, 0.419]}"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "cp_results = {}\nsmodels = (\"svm\",\"rf\")\numodels = (\"kmeans\",\"gmm\")\n\nfor pair in [(smodel, umodel) for smodel in smodels for umodel in umodels]:\n    f, s = run_clustering_pipeline(X_train, y_train, X_test, y_test, smodel=pair[0], umodel=pair[1], best_params=best_params, preprocessor=preprocessor)\n    cp_results[pair[0] + \"-\" + pair[1] + \"-f\"] = f\n    cp_results[pair[0] + \"-\" + pair[1] + \"-s\"] = s\n    \ncp_results"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>svm-kmeans-f</th>\n      <th>svm-kmeans-s</th>\n      <th>svm-gmm-f</th>\n      <th>svm-gmm-s</th>\n      <th>rf-kmeans-f</th>\n      <th>rf-kmeans-s</th>\n      <th>rf-gmm-f</th>\n      <th>rf-gmm-s</th>\n    </tr>\n    <tr>\n      <th>n_clusters</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>0.538</td>\n      <td>0.230</td>\n      <td>0.538</td>\n      <td>0.265</td>\n      <td>0.534</td>\n      <td>0.230</td>\n      <td>0.542</td>\n      <td>0.265</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.538</td>\n      <td>0.186</td>\n      <td>0.538</td>\n      <td>0.279</td>\n      <td>0.538</td>\n      <td>0.186</td>\n      <td>0.538</td>\n      <td>0.279</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.538</td>\n      <td>0.231</td>\n      <td>0.538</td>\n      <td>0.292</td>\n      <td>0.533</td>\n      <td>0.231</td>\n      <td>0.534</td>\n      <td>0.293</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.538</td>\n      <td>0.297</td>\n      <td>0.538</td>\n      <td>0.388</td>\n      <td>0.548</td>\n      <td>0.297</td>\n      <td>0.542</td>\n      <td>0.369</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.538</td>\n      <td>0.334</td>\n      <td>0.538</td>\n      <td>0.419</td>\n      <td>0.512</td>\n      <td>0.335</td>\n      <td>0.538</td>\n      <td>0.419</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "            svm-kmeans-f  svm-kmeans-s  svm-gmm-f  svm-gmm-s  rf-kmeans-f  \\\nn_clusters                                                                  \n3                  0.538         0.230      0.538      0.265        0.534   \n4                  0.538         0.186      0.538      0.279        0.538   \n5                  0.538         0.231      0.538      0.292        0.533   \n6                  0.538         0.297      0.538      0.388        0.548   \n7                  0.538         0.334      0.538      0.419        0.512   \n\n            rf-kmeans-s  rf-gmm-f  rf-gmm-s  \nn_clusters                                   \n3                 0.230     0.542     0.265  \n4                 0.186     0.538     0.279  \n5                 0.231     0.534     0.293  \n6                 0.297     0.542     0.369  \n7                 0.335     0.538     0.419  "
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "## display table of results\ndf_cp = pd.DataFrame(cp_results)\ndf_cp[\"n_clusters\"] = [str(i) for i in np.arange(3, 8)]\ndf_cp.set_index(\"n_clusters\", inplace=True)\ndf_cp.head(n=10)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "`svm-kmeans` performs at baseline while `svm-gmm` performs below. The `random forests` model potentially sees a small improvement with the addition of clusters. This is a fairly small dataset with a small number of features. The utility of adding clustering to the pipeline is generally more apparent in higher dimensional data sets."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## QUESTION 4\n\nRun an experiment to see if you can you improve on your workflow with the addition of re-sampling techniques?"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "def run_clustering_and_resampling_pipeline(X_train, y_train, X_test, y_test, smodel, umodel, best_params, preprocessor):\n    fscores,sscores = [],[]\n    for n_clusters in np.arange(3, 8):\n        \n        if smodel==\"rf\":\n            clf = RandomForestClassifier(n_estimators=best_params['rf__n_estimators'], criterion=best_params['rf__criterion'], max_depth=best_params['rf__max_depth'])\n        elif smodel==\"log\":\n            clf = LogisticRegression(C=best_params['log__C'], penalty=best_params[\"log__penalty\"])\n        elif smodel==\"svm\":\n            clf = SVC(C=best_params['svm__C'], gamma=best_params['svm__gamma'])\n        else:\n            raise Exception(\"invalid supervised learning model\")\n        \n        if umodel==\"kmeans\":\n            cluster = KmeansTransformer(k=n_clusters)\n        elif umodel==\"gmm\":\n            cluster = GmmTransformer(k=n_clusters)\n        else:\n            raise Exception(\"invalid unsupervised learning model\")\n            \n        pipe = pl.Pipeline(steps=[\n            ('pre', preprocessor),\n            ('cluster', cluster),\n            ('smote', SMOTE(random_state=42)),\n            ('clf', clf)])  \n        \n        pipe.fit(X_train, y_train)\n        y_pred = pipe.predict(X_test)\n        \n        fscore = round(f1_score(y_test, y_pred, average='binary'),3)\n        sscore = pipe['cluster'].silhouette_score\n        \n        fscores.append(fscore)\n        sscores.append(sscore)\n        \n    return fscores, sscores"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'svm-kmeans-f': [0.609, 0.609, 0.609, 0.609, 0.609],\n 'svm-kmeans-s': [0.23, 0.186, 0.231, 0.298, 0.335],\n 'svm-gmm-f': [0.609, 0.609, 0.609, 0.609, 0.609],\n 'svm-gmm-s': [0.265, 0.347, 0.343, 0.369, 0.422],\n 'rf-kmeans-f': [0.609, 0.609, 0.6, 0.597, 0.609],\n 'rf-kmeans-s': [0.229, 0.186, 0.231, 0.298, 0.334],\n 'rf-gmm-f': [0.609, 0.613, 0.609, 0.609, 0.609],\n 'rf-gmm-s': [0.265, 0.291, 0.352, 0.352, 0.393]}"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "cp_results = {}\nsmodels = (\"svm\",\"rf\")\numodels = (\"kmeans\",\"gmm\")\n\nfor pair in [(smodel, umodel) for smodel in smodels for umodel in umodels]:\n    f, s = run_clustering_and_resampling_pipeline(X_train, y_train, X_test, y_test, smodel=pair[0], umodel=pair[1], best_params=best_params, preprocessor=preprocessor)\n    cp_results[pair[0] + \"-\" + pair[1] + \"-f\"] = f\n    cp_results[pair[0] + \"-\" + pair[1] + \"-s\"] = s\n    \ncp_results"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>svm-kmeans-f</th>\n      <th>svm-kmeans-s</th>\n      <th>svm-gmm-f</th>\n      <th>svm-gmm-s</th>\n      <th>rf-kmeans-f</th>\n      <th>rf-kmeans-s</th>\n      <th>rf-gmm-f</th>\n      <th>rf-gmm-s</th>\n    </tr>\n    <tr>\n      <th>n_clusters</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>0.609</td>\n      <td>0.230</td>\n      <td>0.609</td>\n      <td>0.265</td>\n      <td>0.609</td>\n      <td>0.229</td>\n      <td>0.609</td>\n      <td>0.265</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.609</td>\n      <td>0.186</td>\n      <td>0.609</td>\n      <td>0.347</td>\n      <td>0.609</td>\n      <td>0.186</td>\n      <td>0.613</td>\n      <td>0.291</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.609</td>\n      <td>0.231</td>\n      <td>0.609</td>\n      <td>0.343</td>\n      <td>0.600</td>\n      <td>0.231</td>\n      <td>0.609</td>\n      <td>0.352</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.609</td>\n      <td>0.298</td>\n      <td>0.609</td>\n      <td>0.369</td>\n      <td>0.597</td>\n      <td>0.298</td>\n      <td>0.609</td>\n      <td>0.352</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.609</td>\n      <td>0.335</td>\n      <td>0.609</td>\n      <td>0.422</td>\n      <td>0.609</td>\n      <td>0.334</td>\n      <td>0.609</td>\n      <td>0.393</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "            svm-kmeans-f  svm-kmeans-s  svm-gmm-f  svm-gmm-s  rf-kmeans-f  \\\nn_clusters                                                                  \n3                  0.609         0.230      0.609      0.265        0.609   \n4                  0.609         0.186      0.609      0.347        0.609   \n5                  0.609         0.231      0.609      0.343        0.600   \n6                  0.609         0.298      0.609      0.369        0.597   \n7                  0.609         0.335      0.609      0.422        0.609   \n\n            rf-kmeans-s  rf-gmm-f  rf-gmm-s  \nn_clusters                                   \n3                 0.229     0.609     0.265  \n4                 0.186     0.613     0.291  \n5                 0.231     0.609     0.352  \n6                 0.298     0.609     0.352  \n7                 0.334     0.609     0.393  "
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "## display table of results\ndf_cp = pd.DataFrame(cp_results)\ndf_cp[\"n_clusters\"] = [str(i) for i in np.arange(3, 8)]\ndf_cp.set_index(\"n_clusters\", inplace=True)\ndf_cp.head(n=10)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Solution Note"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The inclusion of customer profiles does not significantly improve the overall model performance pipeline for either model. There may be some minor improvement depending on the random seed, but since it does not degrade model performance either it can be useful in the context of marketing. The clusters are customer profiles that are tied to predictive performance. The re-sampling does help the random forest classifiers obtain similar performance results to SVM in this case."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}