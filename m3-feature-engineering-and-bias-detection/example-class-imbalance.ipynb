{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Make this notebook run in IBM Watson"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# START CODE BLOCK\n# cos2file - takes an object from Cloud Object Storage and writes it to file on container file system.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: project object defined in project token\n# data_path: the directory to write the file\n# filename: name of the file in COS\n\nimport os\ndef cos2file(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n    open( data_dir + '/' + filename, 'wb').write(p.get_file(filename).read())\n\n# file2cos - takes file on container file system and writes it to an object in Cloud Object Storage.\n# Uses the IBM project_lib library.\n# See https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html\n# Arguments:\n# p: prooject object defined in project token\n# data_path: the directory to read the file from\n# filename: name of the file on container file system\n\nimport os\ndef file2cos(p,data_path,filename):\n    data_dir = p.project_context.home + data_path\n    path_to_file = data_dir + '/' + filename\n    if os.path.exists(path_to_file):\n        file_object = open(path_to_file, 'rb')\n        p.save_data(filename, file_object, set_project_asset=True, overwrite=True)\n    else:\n        print(\"file2cos error: File not found\")\n# END CODE BLOCK"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "cos2file(project, '/data', 'aavail-target.csv')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Setup"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.metrics import median_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_boston\nfrom collections import Counter\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# Pipelines"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "There are an incredible amount of possible workflows for any given data set when we account for transforms, feature engineering, model selection and model tuning. This means that we need a systematic way to compare these workflow variants. This is where pipelines become so useful and it is the consistency of the three interfaces that allow make pipelines like this one a necessary part of the iterative workflow."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "R^2=0.74, MAE=1.46\n"
                }
            ],
            "source": "## load the boston dataset\nboston = load_boston()\nX, y = boston['data'], boston['target']\nfeatures = boston['feature_names']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\npipe = Pipeline([(\"scaler\", StandardScaler()),\n                 (\"featsel\", SelectKBest(k=10)),\n                 (\"rf\",RandomForestRegressor(n_estimators=20))])\n\n## train the data\npipe.fit(X_train,y_train)\n\n## evaluate the model\ny_pred = pipe.predict(X_test)\nprint(r'R^2=%.2f, MAE=%.2f'%(r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Here we are standardizing the data then selecting the 10 best features according to an ANOVA test. These transformed data are then piped into a random forest regression model. See the SelectKBest class to see the other options that are available as a scoring function. It is worth mentioning the three scikit-learn interfaces in combination with pipelines have had such an impact on the data science workflow that Apache Spark now has similar ML pipelines."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Class Imbalance"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>is_subscriber</th>\n      <th>country</th>\n      <th>age</th>\n      <th>customer_name</th>\n      <th>subscriber_type</th>\n      <th>num_streams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>21</td>\n      <td>Kasen Todd</td>\n      <td>aavail_premium</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>singapore</td>\n      <td>30</td>\n      <td>Ensley Garza</td>\n      <td>aavail_unlimited</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>united_states</td>\n      <td>21</td>\n      <td>Lillian Carey</td>\n      <td>aavail_premium</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>united_states</td>\n      <td>20</td>\n      <td>Beau Christensen</td>\n      <td>aavail_basic</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>singapore</td>\n      <td>21</td>\n      <td>Ernesto Gibson</td>\n      <td>aavail_premium</td>\n      <td>23</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   customer_id  is_subscriber        country  age     customer_name  \\\n0            1              1  united_states   21        Kasen Todd   \n1            2              0      singapore   30      Ensley Garza   \n2            3              0  united_states   21     Lillian Carey   \n3            4              1  united_states   20  Beau Christensen   \n4            5              1      singapore   21    Ernesto Gibson   \n\n    subscriber_type  num_streams  \n0    aavail_premium           23  \n1  aavail_unlimited           12  \n2    aavail_premium           22  \n3      aavail_basic           19  \n4    aavail_premium           23  "
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "data_dir = os.path.join(\"..\", \"data\")\ndf = pd.read_csv(os.path.join(data_dir, r\"aavail-target.csv\"))\ndf.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "## pull out the target and remove uneeded columns\n_y = df.pop(\"is_subscriber\")\n\n## switch churn to be the minority class\ny = np.zeros(_y.size)\ny[_y==0] = 1\ndf.drop(columns=[\"customer_id\", \"customer_name\"], inplace=True)"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[(0.0, 569), (1.0, 231)]\n[(0.0, 142), (1.0, 58)]\n"
                }
            ],
            "source": "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, stratify=y)\nprint(sorted(Counter(y_train).items()))\nprint(sorted(Counter(y_test).items()))"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "## transformation pipeline\nnumeric_features = [\"age\", \"num_streams\"]\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\n\ncategorical_features = [\"country\", \"subscriber_type\"]\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    (\"num\", numeric_transformer, numeric_features),\n    (\"cat\", categorical_transformer, categorical_features)\n])\n\n## model pipeline\nclf = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"classifier\", LogisticRegression(solver=\"lbfgs\"))\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "              precision    recall  f1-score   support\n\n  subscriber       0.86      0.84      0.85       142\n       churn       0.63      0.67      0.65        58\n\n    accuracy                           0.79       200\n   macro avg       0.75      0.76      0.75       200\nweighted avg       0.79      0.79      0.79       200\n\n"
                }
            ],
            "source": "clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=[\"subscriber\", \"churn\"]))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Imbalanced Learn"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already up-to-date: imbalanced-learn in /opt/conda/envs/Python36/lib/python3.6/site-packages (0.6.2)\r\nRequirement already satisfied, skipping upgrade: scipy>=0.17 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn) (1.2.0)\r\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn) (0.14.1)\r\nRequirement already satisfied, skipping upgrade: scikit-learn>=0.22 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn) (0.22.2.post1)\r\nRequirement already satisfied, skipping upgrade: numpy>=1.11 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn) (1.15.4)\r\n"
                }
            ],
            "source": "!pip install -U imbalanced-learn"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n"
                }
            ],
            "source": "from sklearn.datasets import make_classification\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Original Target\n[(0, 64), (1, 262), (2, 4674)]\n\nResampled Target\n[(0, 4674), (1, 4674), (2, 4674)]\n"
                }
            ],
            "source": "X, y = make_classification(n_samples=5000, n_features=2, n_informative=2, \n                           n_redundant=0, n_repeated=0, n_classes=3, \n                           n_clusters_per_class=1, \n                           weights=[0.01, 0.05, 0.94], \n                           class_sep=0.8, random_state=0)\n\nprint(\"Original Target\")\nprint(sorted(Counter(y).items()))\n\nros = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = ros.fit_resample(X, y)\n\nprint(\"\\nResampled Target\")\nprint(sorted(Counter(y_resampled).items()))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Compare different methods for imbalancing"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "import imblearn.pipeline as pl\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\n\n## Pipelines\nclf1 = pl.Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"classifier\", LogisticRegression(solver=\"lbfgs\"))\n])\n\nclf2 = pl.Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"ros\", RandomOverSampler(random_state=42)),\n    (\"classifier\", LogisticRegression(solver=\"lbfgs\"))\n    \n])\n\nclf3 = pl.Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"smote\", SMOTE(random_state=42)),\n    (\"classifier\", LogisticRegression(solver=\"lbfgs\"))\n    \n])\n\nfor clf in [clf1, clf2, clf3]:\n    clf.fit(X_train, y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "no sampling\n              precision    recall  f1-score   support\n\n  subscriber       0.86      0.84      0.85       142\n       churn       0.63      0.67      0.65        58\n\n    accuracy                           0.79       200\n   macro avg       0.75      0.76      0.75       200\nweighted avg       0.79      0.79      0.79       200\n\nrandom oversampling\n              precision    recall  f1-score   support\n\n  subscriber       0.87      0.82      0.84       142\n       churn       0.62      0.69      0.65        58\n\n    accuracy                           0.79       200\n   macro avg       0.74      0.76      0.75       200\nweighted avg       0.79      0.79      0.79       200\n\nsmote\n              precision    recall  f1-score   support\n\n  subscriber       0.87      0.82      0.84       142\n       churn       0.62      0.69      0.65        58\n\n    accuracy                           0.79       200\n   macro avg       0.74      0.76      0.75       200\nweighted avg       0.79      0.79      0.79       200\n\n"
                }
            ],
            "source": "for name, clf in [(\"no sampling\", clf1), (\"random oversampling\", clf2), (\"smote\", clf3)]:\n    y_pred = clf.predict(X_test)\n    print(name)\n    print(classification_report(y_test, y_pred, target_names=[\"subscriber\", \"churn\"]))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}